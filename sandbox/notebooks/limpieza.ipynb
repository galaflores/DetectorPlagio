{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Librerias",
   "id": "c904116442e5d82a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:14:28.562893Z",
     "start_time": "2024-05-01T01:14:28.558087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import difflib\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "import re"
   ],
   "id": "8387bd81169556d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:16:05.400208Z",
     "start_time": "2024-05-01T01:15:54.449112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer() #lemmatizer algorithm\n",
    "lancStemmer = LancasterStemmer()  # stemming algorithm Lancaster\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ],
   "id": "2f32d6ca4900e197",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funciones de preprocesamiento a nivel de palabras",
   "id": "44c217ba5a91e35f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:15:01.459099Z",
     "start_time": "2024-05-01T01:15:01.454907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ],
   "id": "5f11159b42ad141e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:15:19.237597Z",
     "start_time": "2024-05-01T01:15:19.233995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_lemmatizer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lemmatizer.lemmatize(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ],
   "id": "55302e16ab1c15e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:16:05.402998Z",
     "start_time": "2024-05-01T01:16:05.401214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ],
   "id": "7367c8ab1258eab6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:19:14.960682Z",
     "start_time": "2024-05-01T01:19:14.957041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_grams(text, ngram, method):\n",
    "    if method == 'lemmatize':\n",
    "        text = get_lemmatizer(text)\n",
    "    elif method == 'stemmer':\n",
    "        text = get_stemmer(text)\n",
    "    else:\n",
    "        raise ValueError('Method not found')\n",
    "        \n",
    "    text = text.split() \n",
    "    if ngram == 0:\n",
    "        raise ValueError('ngram must be greater than 0 and less than 3')\n",
    "    else:\n",
    "        grams = ngrams(text, ngram)  \n",
    "        result = []\n",
    "        for ng in grams:\n",
    "            result.append(' '.join(ng))\n",
    "        return result"
   ],
   "id": "914cc005a8c103e5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:28:41.948328Z",
     "start_time": "2024-05-01T01:28:41.944053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_matrix(text_1, text_2):\n",
    "    text_set = set(text_1 + text_2)\n",
    "    text_list = [text_1, text_2]\n",
    "    matrix = []\n",
    "    for text in text_list:\n",
    "        vector = []\n",
    "        for word in text_set:\n",
    "            if word in text:\n",
    "                vector.append(1 if text.count(word) == 1 else text.count(word))\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        matrix.append(vector)\n",
    "    return matrix"
   ],
   "id": "6cf991b7d50d8879",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:28:45.544701Z",
     "start_time": "2024-05-01T01:28:45.540902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_process(folder_path, ngram, method):\n",
    "    texto_preprocesado = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                grams = get_grams(text, ngram, method)\n",
    "                texto_preprocesado.append((fileid, grams))\n",
    "    return texto_preprocesado"
   ],
   "id": "753b789353bfe4ac",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llamado a las funciones ",
   "id": "98307991ac0a7450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:32:45.048941Z",
     "start_time": "2024-05-01T01:32:44.988924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados\n",
    "preprocess_plagiados = pre_process(folder_path, 3, 'lemmatize')\n",
    "\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "preprocess_originales = pre_process(folder_path_og, 3, 'lemmatize')"
   ],
   "id": "1f2595d2c85e2909",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculo de similitud de coseno por documento",
   "id": "986fb876fc82843e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:37:37.232791Z",
     "start_time": "2024-05-01T01:37:36.373107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Lista para almacenar las similitudes ordenadas\n",
    "resultados = []\n",
    "\n",
    "# Iterar sobre las listas y calcular la similitud de coseno\n",
    "for id_plagiado, (name_plagiado, grams_plagiado) in enumerate(preprocess_plagiados, 1):\n",
    "    for id_original, (name_original, grams_original) in enumerate(preprocess_originales, 1):\n",
    "        \n",
    "        similitud = cosine_similarity(make_matrix(grams_plagiado, grams_original))\n",
    "        if similitud[0][1] != 0.0 and similitud[0][1] >= 0.2:\n",
    "            resultados.append([name_plagiado, name_original, similitud[0][1]])\n",
    "\n",
    "resultados.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Utilizar pprint en lugar de print para un output m√°s ordenado\n",
    "print('Similitud por documento: \\n ')\n",
    "pprint(resultados)"
   ],
   "id": "d35a56e3b73d98ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud por documento: \n",
      " \n",
      "[['FID-04.txt', 'org-045.txt', 0.9097744360902256],\n",
      " ['FID-03.txt', 'org-016.txt', 0.9063031052534367],\n",
      " ['FID-09.txt', 'org-109.txt', 0.8641489186670575],\n",
      " ['FID-08.txt', 'org-079.txt', 0.8619111987560517],\n",
      " ['FID-05.txt', 'org-085.txt', 0.859999999999999],\n",
      " ['FID-02.txt', 'org-104.txt', 0.6631210306331439],\n",
      " ['FID-01.txt', 'org-076.txt', 0.631683937674689],\n",
      " ['FID-06.txt', 'org-043.txt', 0.6000272127355744],\n",
      " ['FID-10.txt', 'org-007.txt', 0.5786876586795143],\n",
      " ['FID-07.txt', 'org-041.txt', 0.577178719558981]]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Desarrollo del modelo",
   "id": "6c244711fd7aaaa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:02:18.075805Z",
     "start_time": "2024-05-01T02:02:18.073125Z"
    }
   },
   "cell_type": "code",
   "source": "from gensim.models import FastText",
   "id": "405824c598b3aa3a",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:07:12.955241Z",
     "start_time": "2024-05-01T02:04:18.281800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Entrenamiento del modelo FastText\n",
    "def train_fasttext_model(folder_path, vector_size=100, window=5, min_count=1, epochs=10):\n",
    "    corpus = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                preprocessed_text = pre_process(folder_path_og, 3, 'lemmatize')\n",
    "                corpus.append(preprocessed_text)\n",
    "    model = FastText(vector_size=vector_size, window=window, min_count=min_count)\n",
    "    model.build_vocab(sentences=corpus)\n",
    "    model.train(sentences=corpus, total_examples=len(corpus), epochs=epochs)\n",
    "    return model"
   ],
   "id": "fc97ad37414ebb7d",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m folder_path_plagiados \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../textos_plagiados\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     17\u001B[0m folder_path_originales \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../docs_originales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 19\u001B[0m fasttext_model \u001B[38;5;241m=\u001B[39m train_fasttext_model(folder_path_originales)\n",
      "Cell \u001B[0;32mIn[32], line 12\u001B[0m, in \u001B[0;36mtrain_fasttext_model\u001B[0;34m(folder_path, vector_size, window, min_count, epochs)\u001B[0m\n\u001B[1;32m     10\u001B[0m             corpus\u001B[38;5;241m.\u001B[39mappend(preprocessed_text)\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m FastText(vector_size\u001B[38;5;241m=\u001B[39mvector_size, window\u001B[38;5;241m=\u001B[39mwindow, min_count\u001B[38;5;241m=\u001B[39mmin_count)\n\u001B[0;32m---> 12\u001B[0m model\u001B[38;5;241m.\u001B[39mbuild_vocab(sentences\u001B[38;5;241m=\u001B[39mcorpus)\n\u001B[1;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(sentences\u001B[38;5;241m=\u001B[39mcorpus, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(corpus), epochs\u001B[38;5;241m=\u001B[39mepochs)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:490\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_vocab\u001B[39m(\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;28mself\u001B[39m, corpus_iterable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, corpus_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, progress_per\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[1;32m    451\u001B[0m         keep_raw_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, trim_rule\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    452\u001B[0m     ):\n\u001B[1;32m    453\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \n\u001B[1;32m    455\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    488\u001B[0m \n\u001B[1;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    491\u001B[0m     total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_vocab(\n\u001B[1;32m    492\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, progress_per\u001B[38;5;241m=\u001B[39mprogress_per, trim_rule\u001B[38;5;241m=\u001B[39mtrim_rule)\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1497\u001B[0m, in \u001B[0;36mWord2Vec._check_corpus_sanity\u001B[0;34m(self, corpus_iterable, corpus_file, passes)\u001B[0m\n\u001B[1;32m   1495\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001B[39;00m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1497\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEither one of corpus_file or corpus_iterable value must be provided\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoth corpus_file and corpus_iterable must not be provided at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:03:57.008556Z",
     "start_time": "2024-05-01T02:03:53.399525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path_plagiados = \"../../textos_plagiados\"\n",
    "folder_path_originales = \"../../docs_originales\"\n",
    "\n",
    "fasttext_model = train_fasttext_model(folder_path_originales)"
   ],
   "id": "9261b2e50a74698",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m folder_path_plagiados \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../textos_plagiados\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m folder_path_originales \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../docs_originales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m fasttext_model \u001B[38;5;241m=\u001B[39m train_fasttext_model(folder_path_originales)\n",
      "Cell \u001B[0;32mIn[30], line 12\u001B[0m, in \u001B[0;36mtrain_fasttext_model\u001B[0;34m(folder_path, vector_size, window, min_count, epochs)\u001B[0m\n\u001B[1;32m     10\u001B[0m             corpus\u001B[38;5;241m.\u001B[39mappend(preprocessed_text)\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m FastText(vector_size\u001B[38;5;241m=\u001B[39mvector_size, window\u001B[38;5;241m=\u001B[39mwindow, min_count\u001B[38;5;241m=\u001B[39mmin_count)\n\u001B[0;32m---> 12\u001B[0m model\u001B[38;5;241m.\u001B[39mbuild_vocab(sentences\u001B[38;5;241m=\u001B[39mcorpus)\n\u001B[1;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(sentences\u001B[38;5;241m=\u001B[39mcorpus, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(corpus), epochs\u001B[38;5;241m=\u001B[39mepochs)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:490\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_vocab\u001B[39m(\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;28mself\u001B[39m, corpus_iterable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, corpus_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, progress_per\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[1;32m    451\u001B[0m         keep_raw_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, trim_rule\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    452\u001B[0m     ):\n\u001B[1;32m    453\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \n\u001B[1;32m    455\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    488\u001B[0m \n\u001B[1;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    491\u001B[0m     total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_vocab(\n\u001B[1;32m    492\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, progress_per\u001B[38;5;241m=\u001B[39mprogress_per, trim_rule\u001B[38;5;241m=\u001B[39mtrim_rule)\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1497\u001B[0m, in \u001B[0;36mWord2Vec._check_corpus_sanity\u001B[0;34m(self, corpus_iterable, corpus_file, passes)\u001B[0m\n\u001B[1;32m   1495\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001B[39;00m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1497\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEither one of corpus_file or corpus_iterable value must be provided\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoth corpus_file and corpus_iterable must not be provided at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
