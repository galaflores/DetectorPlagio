{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a160284983477fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.343693Z",
     "start_time": "2024-05-01T21:36:49.098700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/galafloresgarcia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/galafloresgarcia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import difflib\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizer algorithm\n",
    "lancStemmer = LancasterStemmer()  # stemming algorithm Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cabc55928702054c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.347113Z",
     "start_time": "2024-05-01T21:36:49.344879Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42d531059485839a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.349357Z",
     "start_time": "2024-05-01T21:36:49.347699Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lemmatizer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lemmatizer.lemmatize(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3366f7a6073761d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.351824Z",
     "start_time": "2024-05-01T21:36:49.350299Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe7413594574936f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.354808Z",
     "start_time": "2024-05-01T21:36:49.352255Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_grams(text, ngram, method):\n",
    "    result = []\n",
    "\n",
    "    if method == 'lemmatize':\n",
    "        text = get_lemmatizer(text)\n",
    "        if ngram == 0:  # Si ngram es 0, se retorna el texto completo sin ngramas\n",
    "            text = nltk.sent_tokenize(text)\n",
    "            text = ' '.join(text)\n",
    "            return text\n",
    "\n",
    "        else:\n",
    "            text = text.split()\n",
    "            grams = ngrams(text, ngram)\n",
    "            for ng in grams:\n",
    "                result.append(' '.join(ng))\n",
    "    elif method == 'stemmer':\n",
    "        text = get_stemmer(text)\n",
    "        if ngram == 0:  # Si ngram es 0, se retorna el texto completo sin ngramas\n",
    "            text = nltk.sent_tokenize(text)\n",
    "            text = ' '.join(text)\n",
    "            return text\n",
    "\n",
    "        else:\n",
    "            text = text.split()\n",
    "            grams = ngrams(text, ngram)\n",
    "            for ng in grams:\n",
    "                result.append(' '.join(ng))\n",
    "    else:\n",
    "        raise ValueError('Method not found')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa2adfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Now hes thinkin bout me every night, oh Is it that sweet? I guess so. Say you cant sleep, baby, I know Thats that me, espresso. Move it up, down, left, right, oh Switch it up like Nintendo. Say you can't sleep, baby, I know That's that me, espresso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e783c5598be2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.357325Z",
     "start_time": "2024-05-01T21:36:49.355551Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_sentence(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_words = get_lemmatizer(sentence)\n",
    "        filtered_sentences.append(filtered_words)\n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75a86b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he thinkin bout every night oh sweet',\n",
       " 'guess',\n",
       " 'say cant sleep baby know thats espresso',\n",
       " 'move left right oh switch like nintendo',\n",
       " 'say sleep baby know espresso']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sentence(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aac47f6a85ab8cbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.359904Z",
     "start_time": "2024-05-01T21:36:49.357927Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_docs(folder_path, ngram, method):\n",
    "    tagged_documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            \n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                grams = get_grams(text, ngram, method)\n",
    "                # Ensure words are split into a list of strings and then converted to tuple\n",
    "                words = tuple(word.split() for word in grams)\n",
    "                # Flatten the list of lists into a single list of strings\n",
    "                words = [word for sublist in words for word in sublist]\n",
    "                tagged_documents.append(TaggedDocument(words=words, tags=[fileid]))\n",
    "\n",
    "    return tagged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7c6125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5fa60796b8922db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.361980Z",
     "start_time": "2024-05-01T21:36:49.360362Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_documents):\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, epochs=200,\n",
    "                    dm=0)  # dm=0 for distributed bag of words (DBOW) mode\n",
    "    model.build_vocab(tagged_documents)\n",
    "    model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "701a181f33632fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.363924Z",
     "start_time": "2024-05-01T21:36:49.362412Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_doc2vec(doc1, doc2, model):\n",
    "    vec1 = model.infer_vector(doc1.words)\n",
    "    vec2 = model.infer_vector(doc2.words)\n",
    "    similarity = model.dv.similarity(doc1.tags[0], doc2.tags[0])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentence_disorder(original_sentences, plagio_sentences):\n",
    "    #cantidad de oraciones es diferente, hay desorden\n",
    "    if len(original_sentences) != len(plagio_sentences):\n",
    "        return True\n",
    "    \n",
    "    #verifica si el orden de las oraciones es diferente\n",
    "    for original, plagio in zip(original_sentences, plagio_sentences):\n",
    "        if original != plagio:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_inserted_sentences(og_text, plagio_text):\n",
    "    og_sentences = token_sentence(og_text)\n",
    "    plagio_sentences = token_sentence(plagio_text)\n",
    "    \n",
    "    #si el plagio tiene mas oraciones que el original, hay oraciones insertadas\n",
    "    if len(plagio_sentences) > len(og_sentences):\n",
    "        return True\n",
    "    \n",
    "    #si el plagio tiene menos oraciones que el original, hay oraciones eliminadas\n",
    "    if len(plagio_sentences) < len(og_sentences):\n",
    "        return True\n",
    "    \n",
    "    #verifica si el orden de las oraciones es diferente\n",
    "    if detect_sentence_disorder(og_sentences, plagio_sentences):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def detect_time_change(og_text, plagio_text):\n",
    "    original_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(og_text)) if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(plagio_text)) if pos.startswith('VB')]\n",
    "\n",
    "    # Si la lista de verbos es diferente, hay un cambio de tiempo\n",
    "    if set(original_verbs) != set(suspicious_verbs):\n",
    "        return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_voice_change(og_text, plagio_text):\n",
    "    original_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(og_text)) if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(plagio_text)) if pos.startswith('VB')]\n",
    "\n",
    "    # Si la lista de verbos es diferente, hay un cambio de voz\n",
    "    if set(original_verbs) != set(suspicious_verbs):\n",
    "        return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_paraphrasing(og_text, plagio_text, model):\n",
    "    similarity_threshold = 0.95  # Umbral de similitud para considerar el parafraseo\n",
    "\n",
    "    similarity = calculate_similarity_doc2vec(og_text, plagio_text, model)\n",
    "    if similarity >= similarity_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58c2ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLAGIARISM TYPE\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')  \n",
    "\n",
    "model = train_doc2vec(tagged_originals + tagged_plagiarized)\n",
    "\n",
    "\n",
    "def get_plagiarism_type(doc1, doc2, model):\n",
    "    vec1 = model.infer_vector(doc1.words)\n",
    "    vec2 = model.infer_vector(doc2.words)\n",
    "\n",
    "    plagiarism_type = []\n",
    "    for plagio_doc in tagged_plagiarized:\n",
    "        max_similarity = 0\n",
    "        most_similar = ''\n",
    "        most_similar_doc = ''\n",
    "\n",
    "        # Comparing with each original document\n",
    "        for original_doc in tagged_originals:\n",
    "            similarity = calculate_similarity_doc2vec(plagio_doc, original_doc, model)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar = original_doc.tags[0]\n",
    "                most_similar_doc = original_doc.words\n",
    "\n",
    "    plagiarism_type.append([plagio_doc.tags[0], most_similar, max_similarity, most_similar_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea3ff018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "\n",
    "# Preprocessing original and plagiarized documents\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c8787a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Doc2Vec model\n",
    "model = train_doc2vec(tagged_originals + tagged_plagiarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70848c546b0d322e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:38:24.659628Z",
     "start_time": "2024-05-01T21:37:37.329049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'FID-04.txt' and 'org-045.txt': 99.72%\n",
      "Similarity between 'FID-03.txt' and 'org-016.txt': 99.60%\n",
      "Similarity between 'FID-05.txt' and 'org-085.txt': 96.26%\n",
      "Similarity between 'FID-09.txt' and 'org-109.txt': 93.78%\n",
      "Similarity between 'FID-08.txt' and 'org-079.txt': 92.20%\n",
      "Similarity between 'FID-06.txt' and 'org-043.txt': 91.61%\n",
      "Similarity between 'FID-07.txt' and 'org-041.txt': 89.16%\n",
      "Similarity between 'FID-02.txt' and 'org-104.txt': 87.69%\n",
      "Similarity between 'FID-10.txt' and 'org-007.txt': 86.32%\n",
      "Similarity between 'FID-01.txt' and 'org-076.txt': 77.84%\n"
     ]
    }
   ],
   "source": [
    "#USA MODELO PARA TODOS LOS DOCUMENTOS FINAL\n",
    "\n",
    "\n",
    "# List to store similarity results\n",
    "similarity_results = []\n",
    "#plagiarism_type = '' \n",
    "\n",
    "# Iterating over each plagiarized text\n",
    "for plagio_doc in tagged_plagiarized:\n",
    "    max_similarity = 0\n",
    "    most_similar = ''\n",
    "    most_similar_doc = ''\n",
    "\n",
    "    # Comparing with each original document\n",
    "    for original_doc in tagged_originals:\n",
    "        similarity = calculate_similarity_doc2vec(plagio_doc, original_doc, model)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar = original_doc.tags[0]\n",
    "            most_similar_doc = original_doc.words\n",
    "\n",
    "    similarity_results.append([plagio_doc.tags[0], most_similar, max_similarity, most_similar_doc])\n",
    "\n",
    "        \n",
    "\n",
    "# Sorting results by similarity in descending order\n",
    "similarity_results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Printing results\n",
    "for result in similarity_results:\n",
    "    plagio_title, original_title, similarity_score, original_doc = result\n",
    "    print(f\"Similarity between '{plagio_title}' and '{original_title}': {similarity_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs2(folder_path, ngram, method):\n",
    "    tagged_documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            \n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)  # Tokenizar el texto en oraciones\n",
    "                document_sentences = []  # Lista para almacenar las oraciones del documento\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    grams = get_grams(sentence, ngram, method)\n",
    "                    # Separar las palabras y agregarlas a la lista de oraciones del documento\n",
    "                    words = [word for gram in grams for word in gram.split()]\n",
    "                    document_sentences.append(words)\n",
    "                \n",
    "                tagged_documents.append(TaggedDocument(words=document_sentences, tags=[fileid]))\n",
    "\n",
    "    return tagged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c64956c9632f4f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:42:53.548273Z",
     "start_time": "2024-05-01T21:42:53.311797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titulo: FID-04.txt\n",
      "Similitud entre 'FID-04.txt' y 'org-045.txt': 99.63%\n",
      "{'TP': 9, 'FP': 0, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-04.txt' y 'org-045.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. (Longitud: 113)\n",
      "Cadena plagiada: Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being.\n",
      "\n",
      "Cadena original: In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agentsâ empathic capabilities. (Longitud: 224)\n",
      "Cadena plagiada: In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agentsâ empathic capabilities.\n",
      "\n",
      "Cadena original: In the current state-of-the-art, there are no tools to do that. (Longitud: 63)\n",
      "Cadena plagiada: In the current state-of-the-art, there are no tools to do that.\n",
      "\n",
      "Cadena original: In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. (Longitud: 113)\n",
      "Cadena plagiada: In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy.\n",
      "\n",
      "Cadena original: The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. (Longitud: 111)\n",
      "Cadena plagiada: The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition.\n",
      "\n",
      "Cadena original: Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definitionâan ontologyâof empathy is developed. (Longitud: 208)\n",
      "Cadena plagiada: Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definitionâan ontologyâof empathy is developed.\n",
      "\n",
      "Cadena original: We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. (Longitud: 197)\n",
      "Cadena plagiada: We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa.\n",
      "\n",
      "Cadena original: Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. (Longitud: 203)\n",
      "Cadena plagiada: Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time.\n",
      "\n",
      "Cadena original: The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactionsâbe it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors. (Longitud: 295)\n",
      "Cadena plagiada: The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactionsâbe it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-03.txt\n",
      "Similitud entre 'FID-03.txt' y 'org-016.txt': 99.00%\n",
      "{'TP': 10, 'FP': 0, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-03.txt' y 'org-016.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. (Longitud: 138)\n",
      "Cadena plagiada: At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response.\n",
      "\n",
      "Cadena original: However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. (Longitud: 166)\n",
      "Cadena plagiada: However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc.\n",
      "\n",
      "Cadena original: At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. (Longitud: 245)\n",
      "Cadena plagiada: At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance.\n",
      "\n",
      "Cadena original: This paper designed an EP Vision System (VS) based on AI technology. (Longitud: 68)\n",
      "Cadena plagiada: This paper designed an EP Vision System (VS) based on AI technology.\n",
      "\n",
      "Cadena original: The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. (Longitud: 175)\n",
      "Cadena plagiada: The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions.\n",
      "\n",
      "Cadena original: The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. (Longitud: 191)\n",
      "Cadena plagiada: The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively.\n",
      "\n",
      "Cadena original: However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. (Longitud: 134)\n",
      "Cadena plagiada: However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%.\n",
      "\n",
      "Cadena original: However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. (Longitud: 134)\n",
      "Cadena plagiada: However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%.\n",
      "\n",
      "Cadena original: The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. (Longitud: 230)\n",
      "Cadena plagiada: The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent.\n",
      "\n",
      "Cadena original: It showed the positive relationship between AI technology and EP VS. (Longitud: 68)\n",
      "Cadena plagiada: It showed the positive relationship between AI technology and EP VS.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-05.txt\n",
      "Similitud entre 'FID-05.txt' y 'org-085.txt': 90.62%\n",
      "{'TP': 1, 'FP': 8, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-05.txt' y 'org-085.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: Internet of Things (IoT) based remote healthcare applications provide (Longitud: 69)\n",
      "Cadena plagiada: Internet of Things (IoT) based remote healthcare applications provide\n",
      "\n",
      "Cadena original: s a complex task and diagnosis results  (Longitud: 39)\n",
      "Cadena plagiada: s a complex task and diagnosis results \n",
      "\n",
      "Cadena original: To address this issue, a novel Recommendation System for Cardiovascular Disease Prediction Using IoT Network (DEEP-CARDIO) has been proposed for providing prior diagnosis, treatment, and dietary recommendations for cardiac diseases. (Longitud: 232)\n",
      "Cadena plagiada: To address this issue, a novel Recommendation System for Cardiovascular Disease Prediction Using IoT Network (DEEP-CARDIO) has been proposed for providing prior diagnosis, treatment, and dietary recommendations for cardiac diseases.\n",
      "\n",
      "Cadena original: re collected from the patientâs remotely by using the four bio sensors such as ECG sensor, Pressure sensor, Pulse sensor and Glucose sensor. (Longitud: 142)\n",
      "Cadena plagiada: re collected from the patientâs remotely by using the four bio sensors such as ECG sensor, Pressure sensor, Pulse sensor and Glucose sensor.\n",
      "\n",
      "Cadena original:  the collected data from the IoT sensors to predict and diagnose the disease. (Longitud: 77)\n",
      "Cadena plagiada:  the collected data from the IoT sensors to predict and diagnose the disease.\n",
      "\n",
      "Cadena original: s implemented by using BiGRU (Bidirectional-Gated Recurrent Unit) attention model which diagnose (Longitud: 96)\n",
      "Cadena plagiada: s implemented by using BiGRU (Bidirectional-Gated Recurrent Unit) attention model which diagnose\n",
      "\n",
      "Cadena original:  physical and dietary recommendations to cardiac patients based on the classified data, via user mobile application. (Longitud: 116)\n",
      "Cadena plagiada:  physical and dietary recommendations to cardiac patients based on the classified data, via user mobile application.\n",
      "\n",
      "Cadena original: s validated by Cloud Simulator (CloudSim) using the real-time Framinghamâs and Statlog heart disease dataset. (Longitud: 111)\n",
      "Cadena plagiada: s validated by Cloud Simulator (CloudSim) using the real-time Framinghamâs and Statlog heart disease dataset.\n",
      "\n",
      "Cadena original:  an overall accuracy of 99.90% whereas, the MABC-SVM, HCBDA and MLbPM method achieve (Longitud: 84)\n",
      "Cadena plagiada:  an overall accuracy of 99.90% whereas, the MABC-SVM, HCBDA and MLbPM method achieve\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-08.txt\n",
      "Similitud entre 'FID-08.txt' y 'org-079.txt': 88.39%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 14, 'FP': 7, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-08.txt' y 'org-079.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: ï»¿The main idea of this paper is the substantiation of the methodological approach to the assessment of personnel risks of enterprises based on the application of the fuzzy logic apparatus in order to identify the problems of personnel risk management and provide appropriate recommendations for their solution. (Longitud: 312)\n",
      "Cadena plagiada: ï»¿The main idea of this paper is the substantiation of the methodological approach to the assessment of personnel risks of enterprises based on the application of the fuzzy logic apparatus in order to identify the problems of personnel risk management and provide appropriate recommendations for their solution.\n",
      "\n",
      "Cadena original: The methodological basis of the study is the classic provisions and fundamental works of foreign and domestic scientists, statistical data, the results of our research into the problems of assessing personnel risks of enterprises. (Longitud: 230)\n",
      "Cadena plagiada: The methodological basis of the study is the classic provisions and fundamental works of foreign and domestic scientists, statistical data, the results of our research into the problems of assessing personnel risks of enterprises.\n",
      "\n",
      "Cadena original: s of foreign and domestic scientists, statistical data,  (Longitud: 56)\n",
      "Cadena plagiada: s of foreign and domestic scientists, statistical data, \n",
      "\n",
      "Cadena original: The methods of fuzzy set theory, comparative analysis, scientific abstraction, generalization of scientific experience of modern theoretical research, systemcomplex approach were used. (Longitud: 184)\n",
      "Cadena plagiada: The methods of fuzzy set theory, comparative analysis, scientific abstraction, generalization of scientific experience of modern theoretical research, systemcomplex approach were used.\n",
      "\n",
      "Cadena original:  fuzzy set theory, comparative analysis, scientific abstraction, generalization of  (Longitud: 83)\n",
      "Cadena plagiada:  fuzzy set theory, comparative analysis, scientific abstraction, generalization of \n",
      "\n",
      "Cadena original: The study proposed a methodological approach to assessing the level of personnel risks of an enterprise; numerical experiments were conducted on the basis of a group of construction equipment manufacturers. (Longitud: 206)\n",
      "Cadena plagiada: The study proposed a methodological approach to assessing the level of personnel risks of an enterprise; numerical experiments were conducted on the basis of a group of construction equipment manufacturers.\n",
      "\n",
      "Cadena original:  a group of construction equipment manufacturers. (Longitud: 49)\n",
      "Cadena plagiada:  a group of construction equipment manufacturers.\n",
      "\n",
      "Cadena original: Analysis of the results of assessing the level of personnel risks of enterprises made it possible to identify the problems of managing personnel risks at enterprises Statement of a mathematical problem: the work considers hierarchical fuzzy data, namely: four groups of indicators for assessing the level of personnel risks (quantitative composition â F1, state of qualifications and intellectual potential â F2, staff turnover â F3, motivational system â F4), each of the indicators has a different number of fuzzy coefficients (there are twelve of them in the current work â vi , i=1Ã·12). (Longitud: 601)\n",
      "Cadena plagiada: Analysis of the results of assessing the level of personnel risks of enterprises made it possible to identify the problems of managing personnel risks at enterprises Statement of a mathematical problem: the work considers hierarchical fuzzy data, namely: four groups of indicators for assessing the level of personnel risks (quantitative composition â F1, state of qualifications and intellectual potential â F2, staff turnover â F3, motivational system â F4), each of the indicators has a different number of fuzzy coefficients (there are twelve of them in the current work â vi , i=1Ã·12).\n",
      "\n",
      "Cadena original:  considers hierarchical fuzzy data (Longitud: 34)\n",
      "Cadena plagiada:  considers hierarchical fuzzy data\n",
      "\n",
      "Cadena original: Indicators are functions of fuzzy coefficients: F1 = r(v1, v2, v3); F2 = g(v4,v5, v6, v7); F3 = h(v8, v9, v10,); F4=q(v11, v12). (Longitud: 128)\n",
      "Cadena plagiada: Indicators are functions of fuzzy coefficients: F1 = r(v1, v2, v3); F2 = g(v4,v5, v6, v7); F3 = h(v8, v9, v10,); F4=q(v11, v12).\n",
      "\n",
      "Cadena original: As an output variable, there is a functional â an integrated indicator Int = f(F1, F2, F3, F4) of the personnel risk level, which, in turn, is also a fuzzy value. (Longitud: 164)\n",
      "Cadena plagiada: As an output variable, there is a functional â an integrated indicator Int = f(F1, F2, F3, F4) of the personnel risk level, which, in turn, is also a fuzzy value.\n",
      "\n",
      "Cadena original: Here, the functions r, g, h, q, f are unknown functions of the given variables. (Longitud: 79)\n",
      "Cadena plagiada: Here, the functions r, g, h, q, f are unknown functions of the given variables.\n",
      "\n",
      "Cadena original:  functions r, g, h, q, f a (Longitud: 26)\n",
      "Cadena plagiada:  functions r, g, h, q, f a\n",
      "\n",
      "Cadena original: We have expert evaluations of the change in all input data; as a rule, they vary within three terms: Low (I), Medium (G), High (E). (Longitud: 131)\n",
      "Cadena plagiada: We have expert evaluations of the change in all input data; as a rule, they vary within three terms: Low (I), Medium (G), High (E).\n",
      "\n",
      "Cadena original: Formalized information on each variable can be written as , then for a group of indicators we have: . (Longitud: 101)\n",
      "Cadena plagiada: Formalized information on each variable can be written as , then for a group of indicators we have: .\n",
      "\n",
      "Cadena original: Using a fuzzy system and performing calculations with its help requires the system to have the following structural elements: membership functions of input and output variables, a rule base, and an output mechanism. (Longitud: 215)\n",
      "Cadena plagiada: Using a fuzzy system and performing calculations with its help requires the system to have the following structural elements: membership functions of input and output variables, a rule base, and an output mechanism.\n",
      "\n",
      "Cadena original: s, a rule base, and an output mechanism. (Longitud: 40)\n",
      "Cadena plagiada: s, a rule base, and an output mechanism.\n",
      "\n",
      "Cadena original: These structural elements are the components that will be built when designing a fuzzy system. (Longitud: 94)\n",
      "Cadena plagiada: These structural elements are the components that will be built when designing a fuzzy system.\n",
      "\n",
      "Cadena original: The built mathematical model and the method of its formalization on the basis of FST make it possible to estimate the level of personnel risk at the enterprise, which enables further substantiation of a set of measures to increase the efficiency of its use. (Longitud: 257)\n",
      "Cadena plagiada: The built mathematical model and the method of its formalization on the basis of FST make it possible to estimate the level of personnel risk at the enterprise, which enables further substantiation of a set of measures to increase the efficiency of its use.\n",
      "\n",
      "Cadena original: The constructed system of fuzzy logical inference can be considered intelligent as it uses elements of computational intelligence, in particular, the theory of fuzzy sets. (Longitud: 171)\n",
      "Cadena plagiada: The constructed system of fuzzy logical inference can be considered intelligent as it uses elements of computational intelligence, in particular, the theory of fuzzy sets.\n",
      "\n",
      "Cadena original: The proposed methodological approach to assessing the level of personnel risks of enterprises based on the apparatus of fuzzy logic allows, in contrast to existing ones, to integrate the consideration of both qualitative and quantitative indicators when assessing the level of personnel risks and personnel movement indicators and to significantly increase the efficiency of decision-making under conditions of uncertainty and reduce costs in the event of adverse situations. (Longitud: 475)\n",
      "Cadena plagiada: The proposed methodological approach to assessing the level of personnel risks of enterprises based on the apparatus of fuzzy logic allows, in contrast to existing ones, to integrate the consideration of both qualitative and quantitative indicators when assessing the level of personnel risks and personnel movement indicators and to significantly increase the efficiency of decision-making under conditions of uncertainty and reduce costs in the event of adverse situations.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-09.txt\n",
      "Similitud entre 'FID-09.txt' y 'org-109.txt': 88.08%\n",
      "{'TP': 8, 'FP': 8, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-09.txt' y 'org-109.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original:  of research for pharmaceutical companies and chemical scientists. (Longitud: 66)\n",
      "Cadena plagiada:  of research for pharmaceutical companies and chemical scientists.\n",
      "\n",
      "Cadena original: Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. (Longitud: 117)\n",
      "Cadena plagiada: Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists.\n",
      "\n",
      "Cadena original:  low efficacy, off-target delivery, time consumption, and high cost  (Longitud: 68)\n",
      "Cadena plagiada:  low efficacy, off-target delivery, time consumption, and high cost \n",
      "\n",
      "Cadena original: However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. (Longitud: 145)\n",
      "Cadena plagiada: However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery.\n",
      "\n",
      "Cadena original:  data from genomics, proteomics, microarray data, and clinical trials  (Longitud: 70)\n",
      "Cadena plagiada:  data from genomics, proteomics, microarray data, and clinical trials \n",
      "\n",
      "Cadena original: Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. (Longitud: 149)\n",
      "Cadena plagiada: Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline.\n",
      "\n",
      "Cadena original: Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. (Longitud: 110)\n",
      "Cadena plagiada: Artificial intelligence and machine learning technology play a crucial role in drug discovery and development.\n",
      "\n",
      "Cadena original: In other words, artificial neural networks and deep learning algorithms have modernized the area. (Longitud: 97)\n",
      "Cadena plagiada: In other words, artificial neural networks and deep learning algorithms have modernized the area.\n",
      "\n",
      "Cadena original:  peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structureâactivity relationship, drug repositioning, polypharmacology, and physiochemical activity. (Longitud: 275)\n",
      "Cadena plagiada:  peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structureâactivity relationship, drug repositioning, polypharmacology, and physiochemical activity.\n",
      "\n",
      "Cadena original: Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structureâactivity relationship, drug repositioning, polypharmacology, and physiochemical activity. (Longitud: 386)\n",
      "Cadena plagiada: Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structureâactivity relationship, drug repositioning, polypharmacology, and physiochemical activity.\n",
      "\n",
      "Cadena original: Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. (Longitud: 113)\n",
      "Cadena plagiada: Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field.\n",
      "\n",
      "Cadena original:  artificial intelligence and deep learning  (Longitud: 43)\n",
      "Cadena plagiada:  artificial intelligence and deep learning \n",
      "\n",
      "Cadena original: , novel data mining, curation, and management techniques  (Longitud: 57)\n",
      "Cadena plagiada: , novel data mining, curation, and management techniques \n",
      "\n",
      "Cadena original: Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. (Longitud: 133)\n",
      "Cadena plagiada: Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms.\n",
      "\n",
      "Cadena original:  artificial intelligence and deep learning  (Longitud: 43)\n",
      "Cadena plagiada:  artificial intelligence and deep learning \n",
      "\n",
      "Cadena original: In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. (Longitud: 185)\n",
      "Cadena plagiada: In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-07.txt\n",
      "Similitud entre 'FID-07.txt' y 'org-041.txt': 81.33%\n",
      "{'TP': 0, 'FP': 6, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-07.txt' y 'org-041.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: Human-AI interaction has become an important focus in  (Longitud: 54)\n",
      "Cadena plagiada: Human-AI interaction has become an important focus in \n",
      "\n",
      "Cadena original: In this context, the use of artificial empathy strategies is  (Longitud: 61)\n",
      "Cadena plagiada: In this context, the use of artificial empathy strategies is \n",
      "\n",
      "Cadena original:  affective and social customer experiences. (Longitud: 43)\n",
      "Cadena plagiada:  affective and social customer experiences.\n",
      "\n",
      "Cadena original:  various studies and related literature. (Longitud: 40)\n",
      "Cadena plagiada:  various studies and related literature.\n",
      "\n",
      "Cadena original:  improve the quality of interactions and customer experiences. (Longitud: 62)\n",
      "Cadena plagiada:  improve the quality of interactions and customer experiences.\n",
      "\n",
      "Cadena original: echnologies such as natural language processing, emotion recognition, and sentiment analysis can enable AI to respond more precisely and sensitively to user needs and emotions. (Longitud: 176)\n",
      "Cadena plagiada: echnologies such as natural language processing, emotion recognition, and sentiment analysis can enable AI to respond more precisely and sensitively to user needs and emotions.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-06.txt\n",
      "Similitud entre 'FID-06.txt' y 'org-043.txt': 80.20%\n",
      "{'TP': 0, 'FP': 7, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-06.txt' y 'org-043.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: computer science students at Uppsala University, Sweden. (Longitud: 56)\n",
      "Cadena plagiada: computer science students at Uppsala University, Sweden.\n",
      "\n",
      "Cadena original: participants perceive anthropomorphic chatbots as  (Longitud: 50)\n",
      "Cadena plagiada: participants perceive anthropomorphic chatbots as \n",
      "\n",
      "Cadena original: A semi-structured interview methodology with five students w (Longitud: 60)\n",
      "Cadena plagiada: A semi-structured interview methodology with five students w\n",
      "\n",
      "Cadena original: s manually analyzed using thematic analysis. (Longitud: 44)\n",
      "Cadena plagiada: s manually analyzed using thematic analysis.\n",
      "\n",
      "Cadena original: whether participants perceive anthropomorphic chatbots as humans or machines. (Longitud: 77)\n",
      "Cadena plagiada: whether participants perceive anthropomorphic chatbots as humans or machines.\n",
      "\n",
      "Cadena original: response of chatbots and exit the chatbots without expressing their frustration (Longitud: 79)\n",
      "Cadena plagiada: response of chatbots and exit the chatbots without expressing their frustration\n",
      "\n",
      "Cadena original:  expect more help and politeness if chatbots are more likely to be female. (Longitud: 74)\n",
      "Cadena plagiada:  expect more help and politeness if chatbots are more likely to be female.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-10.txt\n",
      "Similitud entre 'FID-10.txt' y 'org-007.txt': 78.98%\n",
      "{'TP': 0, 'FP': 4, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-10.txt' y 'org-007.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original:  of Artificial Intelligence (AI) technologies in education (Longitud: 58)\n",
      "Cadena plagiada:  of Artificial Intelligence (AI) technologies in education\n",
      "\n",
      "Cadena original:  identify trends and topics related to AI applications in education (AIEd) (Longitud: 74)\n",
      "Cadena plagiada:  identify trends and topics related to AI applications in education (AIEd)\n",
      "\n",
      "Cadena original: ing interest in using AI for educational purposes  (Longitud: 50)\n",
      "Cadena plagiada: ing interest in using AI for educational purposes \n",
      "\n",
      "Cadena original: s the challenges and future directions of AIEd. (Longitud: 47)\n",
      "Cadena plagiada: s the challenges and future directions of AIEd.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-02.txt\n",
      "Similitud entre 'FID-02.txt' y 'org-104.txt': 78.86%\n",
      "{'TP': 5, 'FP': 0, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-02.txt' y 'org-104.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. (Longitud: 113)\n",
      "Cadena plagiada: Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century.\n",
      "\n",
      "Cadena original: Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. (Longitud: 172)\n",
      "Cadena plagiada: Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields.\n",
      "\n",
      "Cadena original: These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. (Longitud: 127)\n",
      "Cadena plagiada: These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go.\n",
      "\n",
      "Cadena original: Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. (Longitud: 135)\n",
      "Cadena plagiada: Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation.\n",
      "\n",
      "Cadena original: We then summarise the applications of ML to medicine. (Longitud: 53)\n",
      "Cadena plagiada: We then summarise the applications of ML to medicine.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Titulo: FID-01.txt\n",
      "Similitud entre 'FID-01.txt' y 'org-076.txt': 72.20%\n",
      "{'TP': 5, 'FP': 1, 'TN': 0, 'FN': 0}\n",
      "Coincidencias para 'FID-01.txt' y 'org-076.txt':\n",
      "----------------------------\n",
      "\n",
      "Cadena original: ï»¿This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. (Longitud: 205)\n",
      "Cadena plagiada: ï»¿This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints.\n",
      "\n",
      "Cadena original: To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. (Longitud: 201)\n",
      "Cadena plagiada: To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics.\n",
      "\n",
      "Cadena original: In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. (Longitud: 173)\n",
      "Cadena plagiada: In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields.\n",
      "\n",
      "Cadena original: Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. (Longitud: 325)\n",
      "Cadena plagiada: Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader.\n",
      "\n",
      "Cadena original: We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). (Longitud: 83)\n",
      "Cadena plagiada: We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB).\n",
      "\n",
      "Cadena original: Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems. (Longitud: 118)\n",
      "Cadena plagiada: Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "TPR (Tasa de Verdaderos Positivos): 1.00\n",
      "FPR (Tasa de Falsos Positivos): 1.00\n",
      "AUC (Área bajo la curva ROC): 0.50\n"
     ]
    }
   ],
   "source": [
    "#------------POR ORACION SIN MODELO -------------\n",
    "\n",
    "\n",
    "def buscar_y_tokenizar(directorio, nombre_archivo):\n",
    "    for filename in os.listdir(directorio):\n",
    "        if filename == nombre_archivo:\n",
    "            filepath = os.path.join(directorio, filename)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                return sentences\n",
    "    return None\n",
    "\n",
    "\n",
    "def encontrar_coincidencias(sentences_originales, sentences_plagiados):\n",
    "    coincidencias = []\n",
    "    # Contadores para la matriz de auc\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "\n",
    "\n",
    "    for sentence_orig in sentences_originales:\n",
    "        for sentence_plag in sentences_plagiados:\n",
    "            matcher = difflib.SequenceMatcher(None, sentence_orig, sentence_plag)\n",
    "            match = matcher.find_longest_match(0, len(sentence_orig), 0, len(sentence_plag))\n",
    "            if match.size > 0:\n",
    "                # Aplicar stemming y eliminar stopwords a las coincidencias antes de contar las palabras\n",
    "                cadena_orig_clean = get_lemmatizer(sentence_orig[match.a:match.a + match.size])\n",
    "                cadena_plag_clean = get_lemmatizer(sentence_plag[match.b:match.b + match.size])\n",
    "                # Contar las palabras en las coincidencias después de aplicar el lemmatizer y eliminar las stopwords\n",
    "                palabras_orig = cadena_orig_clean.split()\n",
    "                palabras_plag = cadena_plag_clean.split()\n",
    "\n",
    "                if len(palabras_orig) > 3 and len(palabras_plag) > 3:  # Solo considerar coincidencias con más de una palabra\n",
    "                    coincidencias.append({\n",
    "                        \"cadena_orig\": sentence_orig[match.a:match.a + match.size],\n",
    "                        \"cadena_plag\": sentence_plag[match.b:match.b + match.size],\n",
    "                        \"longitud\": match.size\n",
    "                    })\n",
    "                    # Actualizar contadores de la matriz de auc\n",
    "                    if sentence_orig == sentence_plag:\n",
    "                      TP += 1\n",
    "                    else:\n",
    "                      FP += 1\n",
    "                \"\"\" #detecta_desorden_oraciones\n",
    "                if detect_sentence_disorder(sentence_orig, sentence_plag) == True:\n",
    "                  plagiarism_type = 'Desorden de oraciones'\n",
    "                elif detect_time_change(sentence_orig, sentence_plag) == True:\n",
    "                  plagiarism_type = 'Cambio de tiempo'\n",
    "                elif sentence_orig == sentence_plag:\n",
    "                  plagiarism_type = 'Insertar o reemplazar' \"\"\"\n",
    "                   \n",
    "            else:\n",
    "              if sentence_orig not in sentences_originales:\n",
    "                TN += 1\n",
    "              else:\n",
    "                FN += 1\n",
    "                \n",
    "    matriz_auc = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN\n",
    "    }\n",
    "    # print(matriz_auc)\n",
    "\n",
    "    return coincidencias, matriz_auc\n",
    "\n",
    "total_coincidencias = []\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_TN = 0\n",
    "total_FN = 0\n",
    "\n",
    "for titulo in similarity_results:\n",
    "    resultados = []\n",
    "    sentences_originales = buscar_y_tokenizar(folder_path_og, titulo[1])\n",
    "    sentences_plagiados = buscar_y_tokenizar(folder_path, titulo[0])\n",
    "    print(f\"Titulo: {titulo[0]}\")\n",
    "\n",
    "    if sentences_originales and sentences_plagiados:\n",
    "        # Similitud = calcular_similitud_ngramas(sentences_originales, sentences_plagiados, 3)\n",
    "        similitud = titulo[2]\n",
    "        print(f\"Similitud entre '{titulo[0]}' y '{titulo[1]}': {similitud * 100:.2f}%\")\n",
    "        coincidencias, matriz_auc = encontrar_coincidencias(sentences_originales, sentences_plagiados)\n",
    "        # Actualizar contadores totales de la matriz de auc\n",
    "        total_TP += matriz_auc['TP']\n",
    "        total_FP += matriz_auc['FP']\n",
    "        total_TN += matriz_auc['TN']\n",
    "        total_FN += matriz_auc['FN']\n",
    "        print(matriz_auc)\n",
    "        print(f\"Coincidencias para '{titulo[0]}' y '{titulo[1]}':\")\n",
    "        total_coincidencias.extend(coincidencias)\n",
    "\n",
    "        print(\"----------------------------\\n\")\n",
    "        for coincidencia in coincidencias:\n",
    "            print(f\"Cadena original: {coincidencia['cadena_orig']} (Longitud: {coincidencia['longitud']})\")\n",
    "            print(f\"Cadena plagiada: {coincidencia['cadena_plag']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"No se encontraron oraciones en '{titulo[0]}' o '{titulo[1]}'\")\n",
    "        print()\n",
    "    print(\"----------------------------\\n\")\n",
    "    \n",
    "\n",
    "# Calculando TPR, FPR y AUC\n",
    "TPR = total_TP / (total_TP + total_FN) if (total_TP + total_FN) != 0 else 0\n",
    "FPR = total_FP / (total_FP + total_TN) if (total_FP + total_TN) != 0 else 0\n",
    "AUC = (1 + TPR - FPR) / 2\n",
    "\n",
    "# Imprimiendo los valores calculados\n",
    "print(f\"TPR (Tasa de Verdaderos Positivos): {TPR:.2f}\")\n",
    "print(f\"FPR (Tasa de Falsos Positivos): {FPR:.2f}\")\n",
    "print(f\"AUC (Área bajo la curva ROC): {AUC:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_time_change(original_text, suspicious_text):\n",
    "    # Tokenización y etiquetado de partes del habla\n",
    "    original_tokens = word_tokenize(original_text)\n",
    "    suspicious_tokens = word_tokenize(suspicious_text)\n",
    "    \n",
    "    original_pos_tags = pos_tag(original_tokens)\n",
    "    suspicious_pos_tags = pos_tag(suspicious_tokens)\n",
    "    \n",
    "    # Extracción de verbos y sus tiempos verbales\n",
    "    original_verbs = [word for word, pos in original_pos_tags if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in suspicious_pos_tags if pos.startswith('VB')]\n",
    "    \n",
    "    # Comparación de tiempos verbales\n",
    "    time_change_detected = False\n",
    "    for original_verb, suspicious_verb in zip(original_verbs, suspicious_verbs):\n",
    "        if original_verb != suspicious_verb:\n",
    "            print(\"Cambio de tiempo verbal detectado:\")\n",
    "            print(\"Original:\", original_verb)\n",
    "            print(\"Sospechoso:\", suspicious_verb)\n",
    "            time_change_detected = True\n",
    "    \n",
    "    if not time_change_detected:\n",
    "        print(\"No se detectaron cambios de tiempo verbal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdda8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_voice_change(original_text, suspicious_text):\n",
    "    # Tokenización y etiquetado de partes del habla\n",
    "    original_tokens = word_tokenize(original_text)\n",
    "    suspicious_tokens = word_tokenize(suspicious_text)\n",
    "    \n",
    "    original_pos_tags = pos_tag(original_tokens)\n",
    "    suspicious_pos_tags = pos_tag(suspicious_tokens)\n",
    "    \n",
    "    # Extracción de verbos y sus formas base\n",
    "    original_verbs = [word for word, pos in original_pos_tags if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in suspicious_pos_tags if pos.startswith('VB')]\n",
    "    \n",
    "    # Determinar si hay un cambio en la voz\n",
    "    voice_change_detected = False\n",
    "    for original_verb, suspicious_verb in zip(original_verbs, suspicious_verbs):\n",
    "        original_voice = detect_verb_voice(original_verb)\n",
    "        suspicious_voice = detect_verb_voice(suspicious_verb)\n",
    "        \n",
    "        if original_voice != suspicious_voice:\n",
    "            print(\"Cambio de voz detectado:\")\n",
    "            print(\"Original:\", original_verb, \"(\", original_voice, \")\")\n",
    "            print(\"Sospechoso:\", suspicious_verb, \"(\", suspicious_voice, \")\")\n",
    "            voice_change_detected = True\n",
    "    \n",
    "    if not voice_change_detected:\n",
    "        print(\"No se detectaron cambios de voz.\")\n",
    "\n",
    "def detect_verb_voice(verb):\n",
    "    \"\"\"\n",
    "    Esta función utiliza una lista simple de verbos auxiliares\n",
    "    para determinar si un verbo está en voz activa o pasiva.\n",
    "    \"\"\"\n",
    "    active_verbs = ['am', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'have', 'has', 'had', 'do', 'does', 'did']\n",
    "    if verb.lower() in active_verbs:\n",
    "        return \"Activa\"\n",
    "    else:\n",
    "        return \"Pasiva\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
