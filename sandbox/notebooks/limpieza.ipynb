{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160284983477fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.343693Z",
     "start_time": "2024-05-01T21:36:49.098700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import difflib\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizer algorithm\n",
    "lancStemmer = LancasterStemmer()  # stemming algorithm Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc55928702054c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.347113Z",
     "start_time": "2024-05-01T21:36:49.344879Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d531059485839a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.349357Z",
     "start_time": "2024-05-01T21:36:49.347699Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lemmatizer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lemmatizer.lemmatize(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3366f7a6073761d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.351824Z",
     "start_time": "2024-05-01T21:36:49.350299Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7413594574936f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.354808Z",
     "start_time": "2024-05-01T21:36:49.352255Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_grams(text, ngram, method):\n",
    "    result = []\n",
    "\n",
    "    if method == 'lemmatize':\n",
    "        text = get_lemmatizer(text)\n",
    "        if ngram == 0:  # Si ngram es 0, se retorna el texto completo sin ngramas\n",
    "            text = nltk.sent_tokenize(text)\n",
    "            text = ' '.join(text)\n",
    "            return text\n",
    "\n",
    "        else:\n",
    "            text = text.split()\n",
    "            grams = ngrams(text, ngram)\n",
    "            for ng in grams:\n",
    "                result.append(' '.join(ng))\n",
    "    elif method == 'stemmer':\n",
    "        text = get_stemmer(text)\n",
    "        if ngram == 0:  # Si ngram es 0, se retorna el texto completo sin ngramas\n",
    "            text = nltk.sent_tokenize(text)\n",
    "            text = ' '.join(text)\n",
    "            return text\n",
    "\n",
    "        else:\n",
    "            text = text.split()\n",
    "            grams = ngrams(text, ngram)\n",
    "            for ng in grams:\n",
    "                result.append(' '.join(ng))\n",
    "    else:\n",
    "        raise ValueError('Method not found')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2adfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Now hes thinkin bout me every night, oh Is it that sweet? I guess so. Say you cant sleep, baby, I know Thats that me, espresso. Move it up, down, left, right, oh Switch it up like Nintendo. Say you can't sleep, baby, I know That's that me, espresso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e783c5598be2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.357325Z",
     "start_time": "2024-05-01T21:36:49.355551Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_sentence(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_words = get_lemmatizer(sentence)\n",
    "        filtered_sentences.append(filtered_words)\n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a86b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sentence(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac47f6a85ab8cbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.359904Z",
     "start_time": "2024-05-01T21:36:49.357927Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_docs(folder_path, ngram, method):\n",
    "    tagged_documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            \n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                grams = get_grams(text, ngram, method)\n",
    "                # Ensure words are split into a list of strings and then converted to tuple\n",
    "                words = tuple(word.split() for word in grams)\n",
    "                # Flatten the list of lists into a single list of strings\n",
    "                words = [word for sublist in words for word in sublist]\n",
    "                tagged_documents.append(TaggedDocument(words=words, tags=[fileid]))\n",
    "\n",
    "    return tagged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60796b8922db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.361980Z",
     "start_time": "2024-05-01T21:36:49.360362Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_documents):\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, epochs=200,\n",
    "                    dm=0)  # dm=0 for distributed bag of words (DBOW) mode\n",
    "    model.build_vocab(tagged_documents)\n",
    "    model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a181f33632fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:36:49.363924Z",
     "start_time": "2024-05-01T21:36:49.362412Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_doc2vec(doc1, doc2, model):\n",
    "    vec1 = model.infer_vector(doc1.words)\n",
    "    vec2 = model.infer_vector(doc2.words)\n",
    "    similarity = model.dv.similarity(doc1.tags[0], doc2.tags[0])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentence_disorder(original_sentences, plagio_sentences):\n",
    "    #cantidad de oraciones es diferente, hay desorden\n",
    "    if len(original_sentences) != len(plagio_sentences):\n",
    "        return True\n",
    "    \n",
    "    #verifica si el orden de las oraciones es diferente\n",
    "    for original, plagio in zip(original_sentences, plagio_sentences):\n",
    "        if original != plagio:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_inserted_sentences(og_text, plagio_text):\n",
    "    og_sentences = token_sentence(og_text)\n",
    "    plagio_sentences = token_sentence(plagio_text)\n",
    "    \n",
    "    #si el plagio tiene mas oraciones que el original, hay oraciones insertadas\n",
    "    if len(plagio_sentences) > len(og_sentences):\n",
    "        return True\n",
    "    \n",
    "    #si el plagio tiene menos oraciones que el original, hay oraciones eliminadas\n",
    "    if len(plagio_sentences) < len(og_sentences):\n",
    "        return True\n",
    "    \n",
    "    #verifica si el orden de las oraciones es diferente\n",
    "    if detect_sentence_disorder(og_sentences, plagio_sentences):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def detect_time_change(og_text, plagio_text):\n",
    "    original_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(og_text)) if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(plagio_text)) if pos.startswith('VB')]\n",
    "\n",
    "    # Si la lista de verbos es diferente, hay un cambio de tiempo\n",
    "    if set(original_verbs) != set(suspicious_verbs):\n",
    "        return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_voice_change(og_text, plagio_text):\n",
    "    original_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(og_text)) if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in nltk.pos_tag(nltk.word_tokenize(plagio_text)) if pos.startswith('VB')]\n",
    "\n",
    "    # Si la lista de verbos es diferente, hay un cambio de voz\n",
    "    if set(original_verbs) != set(suspicious_verbs):\n",
    "        return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_paraphrasing(og_text, plagio_text, model):\n",
    "    similarity_threshold = 0.95  # Umbral de similitud para considerar el parafraseo\n",
    "\n",
    "    similarity = calculate_similarity_doc2vec(og_text, plagio_text, model)\n",
    "    if similarity >= similarity_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ff018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "\n",
    "# Preprocessing original and plagiarized documents\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8787a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Doc2Vec model\n",
    "model = train_doc2vec(tagged_originals + tagged_plagiarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70848c546b0d322e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:38:24.659628Z",
     "start_time": "2024-05-01T21:37:37.329049Z"
    }
   },
   "outputs": [],
   "source": [
    "#USA MODELO PARA TODOS LOS DOCUMENTOS FINAL\n",
    "\n",
    "\n",
    "# List to store similarity results\n",
    "similarity_results = []\n",
    "#plagiarism_type = '' \n",
    "\n",
    "# Iterating over each plagiarized text\n",
    "for plagio_doc in tagged_plagiarized:\n",
    "    max_similarity = 0\n",
    "    most_similar = ''\n",
    "    most_similar_doc = ''\n",
    "\n",
    "    # Comparing with each original document\n",
    "    for original_doc in tagged_originals:\n",
    "        similarity = calculate_similarity_doc2vec(plagio_doc, original_doc, model)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar = original_doc.tags[0]\n",
    "            most_similar_doc = original_doc.words\n",
    "\n",
    "    similarity_results.append([plagio_doc.tags[0], most_similar, max_similarity, most_similar_doc])\n",
    "\n",
    "        \n",
    "\n",
    "# Sorting results by similarity in descending order\n",
    "similarity_results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Printing results\n",
    "for result in similarity_results:\n",
    "    plagio_title, original_title, similarity_score, original_doc = result\n",
    "    print(f\"Similarity between '{plagio_title}' and '{original_title}': {similarity_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs2(folder_path, ngram, method):\n",
    "    tagged_documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            \n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)  # Tokenizar el texto en oraciones\n",
    "                document_sentences = []  # Lista para almacenar las oraciones del documento\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    grams = get_grams(sentence, ngram, method)\n",
    "                    # Separar las palabras y agregarlas a la lista de oraciones del documento\n",
    "                    words = [word for gram in grams for word in gram.split()]\n",
    "                    document_sentences.append(words)\n",
    "                \n",
    "                tagged_documents.append(TaggedDocument(words=document_sentences, tags=[fileid]))\n",
    "\n",
    "    return tagged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7837ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener n-gramas preprocesados\n",
    "folder_path = \"../../textos_plagiados\"  # Ruta de la carpeta con los textos plagiados)\n",
    "folder_path_og = \"../../docs_originales\"  # Ruta de la carpeta con los textos originales\n",
    "\n",
    "tagged_originals = preprocess_docs(folder_path_og, 1, 'lemmatize')\n",
    "tagged_plagiarized = preprocess_docs(folder_path, 1, 'lemmatize')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_doc2vec(doc1, doc2, model):\n",
    "    vec1 = model.infer_vector(doc1.words)\n",
    "    vec2 = model.infer_vector(doc2.words)\n",
    "    similarity = model.dv.similarity(doc1.tags[0], doc2.tags[0])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4918719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_documents):\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, epochs=200, dm=0)  # dm=0 for distributed bag of words (DBOW) mode\n",
    "    model.build_vocab(tagged_documents)\n",
    "    model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_y_tokenizar(directorio, nombre_archivo):\n",
    "    for filename in os.listdir(directorio):\n",
    "        if filename == nombre_archivo:\n",
    "            filepath = os.path.join(directorio, filename)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                return sentences\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encontrar_coincidencias(sentences_originales, sentences_plagiados, model):\n",
    "    coincidencias = []\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for sentence_orig in sentences_originales:\n",
    "        for sentence_plag in sentences_plagiados:\n",
    "            similarity = calculate_similarity_doc2vec(sentence_orig, sentence_plag, model)\n",
    "            if similarity > 0.4:  # Define el umbral adecuado para la similitud\n",
    "                coincidencias.append({\n",
    "                    \"cadena_orig\": sentence_orig,\n",
    "                    \"cadena_plag\": sentence_plag,\n",
    "                    \"similitud\": similarity\n",
    "                })\n",
    "                if sentence_orig == sentence_plag:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            else:\n",
    "                if sentence_orig not in sentences_plagiados:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "\n",
    "    matriz_auc = {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN}\n",
    "    return coincidencias, matriz_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    tagged_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tagged_sentence = TaggedDocument(words=nltk.word_tokenize(sentence.lower()), tags=[str(i)])\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ebb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_coincidencias = []\n",
    "similarity_results = []\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_TN = 0\n",
    "total_FN = 0\n",
    "\n",
    "for titulo in similarity_results:\n",
    "    resultados = []\n",
    "    sentences_originales = buscar_y_tokenizar(folder_path_og, titulo[1])\n",
    "    sentences_plagiados = buscar_y_tokenizar(folder_path, titulo[0])\n",
    "    print(f\"Titulo: {titulo[0]}\")\n",
    "\n",
    "    if sentences_originales and sentences_plagiados:\n",
    "        tagged_sentences_originales = preprocess_sentences(sentences_originales)\n",
    "        tagged_sentences_plagiados = preprocess_sentences(sentences_plagiados)\n",
    "        model = train_doc2vec(tagged_sentences_originales + tagged_sentences_plagiados)\n",
    "        similitud = titulo[2]\n",
    "        print(f\"Similitud entre '{titulo[0]}' y '{titulo[1]}': {similitud * 100:.2f}%\")\n",
    "        coincidencias, matriz_auc = encontrar_coincidencias(tagged_sentences_originales, tagged_sentences_plagiados, model)\n",
    "        total_TP += matriz_auc['TP']\n",
    "        total_FP += matriz_auc['FP']\n",
    "        total_TN += matriz_auc['TN']\n",
    "        total_FN += matriz_auc['FN']\n",
    "        total_coincidencias.extend(coincidencias)\n",
    "\n",
    "        print(f\"Coincidencias para '{titulo[0]}' y '{titulo[1]}':\")\n",
    "        for coincidencia in coincidencias:\n",
    "            print(f\"Cadena original: {coincidencia['cadena_orig']} (Similitud: {coincidencia['similitud']})\")\n",
    "            print(f\"Cadena plagiada: {coincidencia['cadena_plag']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"No se encontraron oraciones en '{titulo[0]}' o '{titulo[1]}'\")\n",
    "        print()\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "# Calculando TPR, FPR y AUC\n",
    "TPR = total_TP / (total_TP + total_FN) if (total_TP + total_FN) != 0 else 0\n",
    "FPR = total_FP / (total_FP + total_TN) if (total_FP + total_TN) != 0 else 0\n",
    "AUC = (1 + TPR - FPR) / 2\n",
    "\n",
    "# Imprimiendo los valores calculados\n",
    "print(f\"TPR (Tasa de Verdaderos Positivos): {TPR:.2f}\")\n",
    "print(f\"FPR (Tasa de Falsos Positivos): {FPR:.2f}\")\n",
    "print(f\"AUC (Área bajo la curva ROC): {AUC:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64956c9632f4f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T21:42:53.548273Z",
     "start_time": "2024-05-01T21:42:53.311797Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------POR ORACION SIN MODELO -------------\n",
    "\n",
    "\n",
    "def buscar_y_tokenizar(directorio, nombre_archivo):\n",
    "    for filename in os.listdir(directorio):\n",
    "        if filename == nombre_archivo:\n",
    "            filepath = os.path.join(directorio, filename)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                return sentences\n",
    "    return None\n",
    "\n",
    "\n",
    "def encontrar_coincidencias(sentences_originales, sentences_plagiados):\n",
    "    coincidencias = []\n",
    "    # Contadores para la matriz de auc\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "\n",
    "    for sentence_orig in sentences_originales:\n",
    "        for sentence_plag in sentences_plagiados:\n",
    "            matcher = difflib.SequenceMatcher(None, sentence_orig, sentence_plag)\n",
    "            print('MATCHER', matcher)\n",
    "            match = matcher.find_longest_match(0, len(sentence_orig), 0, len(sentence_plag))\n",
    "            print('MATCH', match)\n",
    "            if match.size > 0:\n",
    "                # Aplicar lemmatization y eliminar stopwords a las coincidencias antes de contar las palabras\n",
    "                cadena_orig_clean = get_lemmatizer(sentence_orig[match.a:match.a + match.size])\n",
    "                cadena_plag_clean = get_lemmatizer(sentence_plag[match.b:match.b + match.size])\n",
    "                # Contar las palabras en las coincidencias después de aplicar el lemmatizer y eliminar las stopwords\n",
    "                palabras_orig = cadena_orig_clean.split()\n",
    "                palabras_plag = cadena_plag_clean.split()\n",
    "\n",
    "                if len(palabras_orig) > 3 and len(palabras_plag) > 3:  # Solo considerar coincidencias con más de una palabra\n",
    "                    coincidencias.append({\n",
    "                        \"cadena_orig\": sentence_orig[match.a:match.a + match.size],\n",
    "                        \"cadena_plag\": sentence_plag[match.b:match.b + match.size],\n",
    "                        \"longitud\": match.size\n",
    "                    })\n",
    "                    # Actualizar contadores de la matriz de auc\n",
    "                    if sentence_orig == sentence_plag:\n",
    "                      TP += 1\n",
    "                    else:\n",
    "                      FP += 1\n",
    "                \"\"\" #detecta_desorden_oraciones\n",
    "                if detect_sentence_disorder(sentence_orig, sentence_plag) == True:\n",
    "                  plagiarism_type = 'Desorden de oraciones'\n",
    "                elif detect_time_change(sentence_orig, sentence_plag) == True:\n",
    "                  plagiarism_type = 'Cambio de tiempo'\n",
    "                elif sentence_orig == sentence_plag:\n",
    "                  plagiarism_type = 'Insertar o reemplazar' \"\"\"\n",
    "                   \n",
    "            else:\n",
    "              if sentence_orig not in sentences_originales:\n",
    "                TN += 1\n",
    "              else:\n",
    "                FN += 1\n",
    "                \n",
    "    matriz_auc = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN\n",
    "    }\n",
    "    # print(matriz_auc)\n",
    "\n",
    "    return coincidencias, matriz_auc\n",
    "\n",
    "total_coincidencias = []\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_TN = 0\n",
    "total_FN = 0\n",
    "\n",
    "for titulo in similarity_results:\n",
    "    resultados = []\n",
    "    sentences_originales = buscar_y_tokenizar(folder_path_og, titulo[1])\n",
    "    sentences_plagiados = buscar_y_tokenizar(folder_path, titulo[0])\n",
    "    print(f\"Titulo: {titulo[0]}\")\n",
    "\n",
    "    if sentences_originales and sentences_plagiados:\n",
    "        similitud = titulo[2]\n",
    "        print(f\"Similitud entre '{titulo[0]}' y '{titulo[1]}': {similitud * 100:.2f}%\")\n",
    "        coincidencias, matriz_auc = encontrar_coincidencias(sentences_originales, sentences_plagiados)\n",
    "        # Actualizar contadores totales de la matriz de auc\n",
    "        total_TP += matriz_auc['TP']\n",
    "        total_FP += matriz_auc['FP']\n",
    "        total_TN += matriz_auc['TN']\n",
    "        total_FN += matriz_auc['FN']\n",
    "        print(matriz_auc)\n",
    "        print(f\"Coincidencias para '{titulo[0]}' y '{titulo[1]}':\")\n",
    "        total_coincidencias.extend(coincidencias)\n",
    "\n",
    "        print(\"----------------------------\\n\")\n",
    "        for coincidencia in coincidencias:\n",
    "            print(f\"Cadena original: {coincidencia['cadena_orig']} (Longitud: {coincidencia['longitud']})\")\n",
    "            print(f\"Cadena plagiada: {coincidencia['cadena_plag']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"No se encontraron oraciones en '{titulo[0]}' o '{titulo[1]}'\")\n",
    "        print()\n",
    "    print(\"----------------------------\\n\")\n",
    "    \n",
    "\n",
    "# Calculando TPR, FPR y AUC\n",
    "TPR = total_TP / (total_TP + total_FN) if (total_TP + total_FN) != 0 else 0\n",
    "FPR = total_FP / (total_FP + total_TN) if (total_FP + total_TN) != 0 else 0\n",
    "AUC = (1 + TPR - FPR) / 2\n",
    "\n",
    "# Imprimiendo los valores calculados\n",
    "print(f\"TPR (Tasa de Verdaderos Positivos): {TPR:.2f}\")\n",
    "print(f\"FPR (Tasa de Falsos Positivos): {FPR:.2f}\")\n",
    "print(f\"AUC (Área bajo la curva ROC): {AUC:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_time_change(original_text, suspicious_text):\n",
    "    # Tokenización y etiquetado de partes del habla\n",
    "    original_tokens = word_tokenize(original_text)\n",
    "    suspicious_tokens = word_tokenize(suspicious_text)\n",
    "    \n",
    "    original_pos_tags = pos_tag(original_tokens)\n",
    "    suspicious_pos_tags = pos_tag(suspicious_tokens)\n",
    "    \n",
    "    # Extracción de verbos y sus tiempos verbales\n",
    "    original_verbs = [word for word, pos in original_pos_tags if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in suspicious_pos_tags if pos.startswith('VB')]\n",
    "    \n",
    "    # Comparación de tiempos verbales\n",
    "    time_change_detected = False\n",
    "    for original_verb, suspicious_verb in zip(original_verbs, suspicious_verbs):\n",
    "        if original_verb != suspicious_verb:\n",
    "            print(\"Cambio de tiempo verbal detectado:\")\n",
    "            print(\"Original:\", original_verb)\n",
    "            print(\"Sospechoso:\", suspicious_verb)\n",
    "            time_change_detected = True\n",
    "    \n",
    "    if not time_change_detected:\n",
    "        print(\"No se detectaron cambios de tiempo verbal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdda8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_voice_change(original_text, suspicious_text):\n",
    "    # Tokenización y etiquetado de partes del habla\n",
    "    original_tokens = word_tokenize(original_text)\n",
    "    suspicious_tokens = word_tokenize(suspicious_text)\n",
    "    \n",
    "    original_pos_tags = pos_tag(original_tokens)\n",
    "    suspicious_pos_tags = pos_tag(suspicious_tokens)\n",
    "    \n",
    "    # Extracción de verbos y sus formas base\n",
    "    original_verbs = [word for word, pos in original_pos_tags if pos.startswith('VB')]\n",
    "    suspicious_verbs = [word for word, pos in suspicious_pos_tags if pos.startswith('VB')]\n",
    "    \n",
    "    # Determinar si hay un cambio en la voz\n",
    "    voice_change_detected = False\n",
    "    for original_verb, suspicious_verb in zip(original_verbs, suspicious_verbs):\n",
    "        original_voice = detect_verb_voice(original_verb)\n",
    "        suspicious_voice = detect_verb_voice(suspicious_verb)\n",
    "        \n",
    "        if original_voice != suspicious_voice:\n",
    "            print(\"Cambio de voz detectado:\")\n",
    "            print(\"Original:\", original_verb, \"(\", original_voice, \")\")\n",
    "            print(\"Sospechoso:\", suspicious_verb, \"(\", suspicious_voice, \")\")\n",
    "            voice_change_detected = True\n",
    "    \n",
    "    if not voice_change_detected:\n",
    "        print(\"No se detectaron cambios de voz.\")\n",
    "\n",
    "def detect_verb_voice(verb):\n",
    "    \"\"\"\n",
    "    Esta función utiliza una lista simple de verbos auxiliares\n",
    "    para determinar si un verbo está en voz activa o pasiva.\n",
    "    \"\"\"\n",
    "    active_verbs = ['am', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'have', 'has', 'had', 'do', 'does', 'did']\n",
    "    if verb.lower() in active_verbs:\n",
    "        return \"Activa\"\n",
    "    else:\n",
    "        return \"Pasiva\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
