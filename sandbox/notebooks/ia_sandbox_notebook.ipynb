{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:14.731764Z",
     "start_time": "2024-05-02T20:37:13.866110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import difflib\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "import re"
   ],
   "id": "fa5056caea261fd0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:14.739127Z",
     "start_time": "2024-05-02T20:37:14.732563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import FastText\n",
    "import nltk\n",
    "import os\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "# Descargar recursos de NLTK si no los tienes\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar el lematizador y el stemmer de NLTK\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lancStemmer = nltk.stem.LancasterStemmer()\n",
    "\n",
    "# Función para eliminar stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para lematizar texto\n",
    "def get_lemmatizer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lemmatizer.lemmatize(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para stemming\n",
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para obtener n-gramas\n",
    "def get_grams(text, ngram, method):\n",
    "    if method == 'lemmatize':\n",
    "        text = get_lemmatizer(text)\n",
    "    elif method == 'stemmer':\n",
    "        text = get_stemmer(text)\n",
    "    else:\n",
    "        raise ValueError('Method not found')\n",
    "        \n",
    "    text = text.split() \n",
    "    if ngram == 0:\n",
    "        raise ValueError('ngram must be greater than 0 and less than 3')\n",
    "    else:\n",
    "        grams = ngrams(text, ngram)  \n",
    "        result = []\n",
    "        for ng in grams:\n",
    "            result.append(' '.join(ng))\n",
    "        return result\n",
    "\n",
    "# Función para preprocesar los documentos\n",
    "def preprocess_documents(folder_path, ngram, method):\n",
    "    documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                grams = get_grams(text, ngram, method)\n",
    "                documents.append(grams)\n",
    "    return documents\n",
    "\n",
    "# Entrenamiento del modelo FastText\n",
    "def train_fasttext_model(documents, vector_size=100, window=5, min_count=1, epochs=10):\n",
    "    model = FastText(vector_size=vector_size, window=window, min_count=min_count)\n",
    "    model.build_vocab(corpus_iterable=documents)  # Pasar documentos como corpus_iterable\n",
    "    model.train(corpus_iterable=documents, total_examples=len(documents), epochs=epochs)\n",
    "    return model"
   ],
   "id": "5f9d19f5c1d019d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:15.225662Z",
     "start_time": "2024-05-02T20:37:14.739755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Ruta de los documentos originales\n",
    "folder_path_originales = \"../../docs_originales\"\n",
    "# Preprocesar los documentos originales\n",
    "documents_originales = preprocess_documents(folder_path_originales, ngram=3, method='lemmatize')"
   ],
   "id": "a8b52a3365bf3d43",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:19.508150Z",
     "start_time": "2024-05-02T20:37:15.226984Z"
    }
   },
   "cell_type": "code",
   "source": "fasttext_model = train_fasttext_model(documents_originales)",
   "id": "1be282b0d110d2a2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:19.511143Z",
     "start_time": "2024-05-02T20:37:19.508831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calcular embeddings para n-gramas\n",
    "def calculate_embeddings(text, model):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in model.wv.key_to_index:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            # Si la palabra no está en el vocabulario del modelo, generamos un vector de ceros\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return embeddings\n",
    "\n",
    "# Calcular la similitud de coseno entre los documentos plagiados y originales\n",
    "def calculate_similarity(embeddings_plagiado, embeddings_original):\n",
    "    similitud = cosine_similarity(embeddings_plagiado, embeddings_original)\n",
    "    return similitud[0][0]"
   ],
   "id": "c89077849bbc64ef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:19.518613Z",
     "start_time": "2024-05-02T20:37:19.511727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path_plagio= \"../../textos_plagiados\"\n",
    "preprocessed_text_plagiado = preprocess_documents(folder_path_plagio, ngram=3, method='lemmatize')\n",
    "\n"
   ],
   "id": "3d9da53102674b40",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:37:19.765139Z",
     "start_time": "2024-05-02T20:37:19.519174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Lista para almacenar las similitudes ordenadas\n",
    "resultados = []\n",
    "folder_path_plagio= \"../../textos_plagiados\" \n",
    "# Preprocesar el texto plagiado con los mismos argumentos que los documentos originales\n",
    "preprocessed_text_plagiado = preprocess_documents(folder_path_plagio, ngram=3, method='lemmatize')\n",
    "embeddings_plagiado = calculate_embeddings(preprocessed_text_plagiado, fasttext_model)\n",
    "\n",
    "# Preprocesar el texto original con los mismos argumentos que los documentos originales\n",
    "\n",
    "embeddings_original = calculate_embeddings(preprocessed_text_original, fasttext_model)\n",
    "\n",
    "# Calcular la similitud de coseno entre los documentos\n",
    "similitud = calculate_similarity(embeddings_plagiado, embeddings_original)\n",
    "if similitud >= 0.2:  # Ajusta el umbral de similitud según tus necesidades\n",
    "    resultados.append([fileid_plagiado, fileid_original, similitud])\n",
    "\n",
    "# Ordenar los resultados por similitud descendente\n",
    "resultados.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Imprimir resultados\n",
    "print('Similitud por documento: \\n')\n",
    "for resultado in resultados:\n",
    "    print(f\"{resultado[0]} - {resultado[1]}: {resultado[2]}\")"
   ],
   "id": "f260c68b073835f8",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Preprocesar el texto plagiado con los mismos argumentos que los documentos originales\u001B[39;00m\n\u001B[1;32m      5\u001B[0m preprocessed_text_plagiado \u001B[38;5;241m=\u001B[39m preprocess_documents(folder_path_plagio, ngram\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlemmatize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m embeddings_plagiado \u001B[38;5;241m=\u001B[39m calculate_embeddings(preprocessed_text_plagiado, fasttext_model)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Preprocesar el texto original con los mismos argumentos que los documentos originales\u001B[39;00m\n\u001B[1;32m     10\u001B[0m embeddings_original \u001B[38;5;241m=\u001B[39m calculate_embeddings(preprocessed_text_original, fasttext_model)\n",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m, in \u001B[0;36mcalculate_embeddings\u001B[0;34m(text, model)\u001B[0m\n\u001B[1;32m      3\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text:\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mkey_to_index:\n\u001B[1;32m      6\u001B[0m         embeddings\u001B[38;5;241m.\u001B[39mappend(model\u001B[38;5;241m.\u001B[39mwv[word])\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;66;03m# Si la palabra no está en el vocabulario del modelo, generamos un vector de ceros\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
