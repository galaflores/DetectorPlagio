{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import difflib\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "import re"
   ],
   "id": "fa5056caea261fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:37:53.478034Z",
     "start_time": "2024-05-01T02:37:53.469833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import FastText\n",
    "import nltk\n",
    "import os\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "# Descargar recursos de NLTK si no los tienes\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar el lematizador y el stemmer de NLTK\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lancStemmer = nltk.stem.LancasterStemmer()\n",
    "\n",
    "# Función para eliminar stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para lematizar texto\n",
    "def get_lemmatizer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lemmatizer.lemmatize(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para stemming\n",
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto\n",
    "\n",
    "# Función para obtener n-gramas\n",
    "def get_grams(text, ngram, method):\n",
    "    if method == 'lemmatize':\n",
    "        text = get_lemmatizer(text)\n",
    "    elif method == 'stemmer':\n",
    "        text = get_stemmer(text)\n",
    "    else:\n",
    "        raise ValueError('Method not found')\n",
    "        \n",
    "    text = text.split() \n",
    "    if ngram == 0:\n",
    "        raise ValueError('ngram must be greater than 0 and less than 3')\n",
    "    else:\n",
    "        grams = ngrams(text, ngram)  \n",
    "        result = []\n",
    "        for ng in grams:\n",
    "            result.append(' '.join(ng))\n",
    "        return result\n",
    "\n",
    "# Función para preprocesar los documentos\n",
    "def preprocess_documents(folder_path, ngram, method):\n",
    "    documents = []\n",
    "    for fileid in os.listdir(folder_path):\n",
    "        if fileid.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, fileid)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                grams = get_grams(text, ngram, method)\n",
    "                documents.append(grams)\n",
    "    return documents\n",
    "\n",
    "# Entrenamiento del modelo FastText\n",
    "def train_fasttext_model(documents, vector_size=100, window=5, min_count=1, epochs=10):\n",
    "    model = FastText(vector_size=vector_size, window=window, min_count=min_count)\n",
    "    model.build_vocab(corpus_iterable=documents)  # Pasar documentos como corpus_iterable\n",
    "    model.train(corpus_iterable=documents, total_examples=len(documents), epochs=epochs)\n",
    "    return model"
   ],
   "id": "5f9d19f5c1d019d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sergiogonzalez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 287
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Ruta de los documentos originales\n",
    "folder_path_originales = \"../../docs_originales\"\n",
    "# Preprocesar los documentos originales\n",
    "documents_originales = preprocess_documents(folder_path_originales, ngram=3, method='lemmatize')"
   ],
   "id": "a8b52a3365bf3d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:38:04.863096Z",
     "start_time": "2024-05-01T02:38:00.479249Z"
    }
   },
   "cell_type": "code",
   "source": "fasttext_model = train_fasttext_model(documents_originales)",
   "id": "1be282b0d110d2a2",
   "outputs": [],
   "execution_count": 289
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:53:42.951917Z",
     "start_time": "2024-05-01T02:53:42.948566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calcular embeddings para n-gramas\n",
    "def calculate_embeddings(text, model):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in model.wv.key_to_index:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            # Si la palabra no está en el vocabulario del modelo, generamos un vector de ceros\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return embeddings\n",
    "\n",
    "# Calcular la similitud de coseno entre los documentos plagiados y originales\n",
    "def calculate_similarity(embeddings_plagiado, embeddings_original):\n",
    "    similitud = cosine_similarity(embeddings_plagiado, embeddings_original)\n",
    "    return similitud[0][0]"
   ],
   "id": "c89077849bbc64ef",
   "outputs": [],
   "execution_count": 322
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:53:45.055850Z",
     "start_time": "2024-05-01T02:53:45.003102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path_plagio= \"../../textos_plagiados\"\n",
    "preprocessed_text_plagiado = preprocess_documents(folder_path_plagio, ngram=3, method='lemmatize')\n",
    "\n"
   ],
   "id": "3d9da53102674b40",
   "outputs": [],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T03:12:56.865548Z",
     "start_time": "2024-05-01T03:12:56.838524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Lista para almacenar las similitudes ordenadas\n",
    "resultados = []\n",
    "folder_path_plagio= \"../../textos_plagiados\" \n",
    "# Preprocesar el texto plagiado con los mismos argumentos que los documentos originales\n",
    "preprocessed_text_plagiado = preprocess_documents(folder_path_plagio, ngram=3, method='lemmatize')\n",
    "embeddings_plagiado = calculate_embeddings(preprocessed_text_plagiado, fasttext_model)\n",
    "\n",
    "# Preprocesar el texto original con los mismos argumentos que los documentos originales\n",
    "\n",
    "embeddings_original = calculate_embeddings(preprocessed_text_original, fasttext_model)\n",
    "\n",
    "# Calcular la similitud de coseno entre los documentos\n",
    "similitud = calculate_similarity(embeddings_plagiado, embeddings_original)\n",
    "if similitud >= 0.2:  # Ajusta el umbral de similitud según tus necesidades\n",
    "    resultados.append([fileid_plagiado, fileid_original, similitud])\n",
    "\n",
    "# Ordenar los resultados por similitud descendente\n",
    "resultados.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Imprimir resultados\n",
    "print('Similitud por documento: \\n')\n",
    "for resultado in resultados:\n",
    "    print(f\"{resultado[0]} - {resultado[1]}: {resultado[2]}\")"
   ],
   "id": "f260c68b073835f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study conducted investigate', 'conducted investigate empathy', 'investigate empathy human', 'empathy human chatbot', 'human chatbot interaction', 'chatbot interaction among', 'interaction among computer', 'among computer science', 'computer science student', 'science student uppsala', 'student uppsala university', 'uppsala university sweden', 'university sweden done', 'sweden done exploring', 'done exploring participant', 'exploring participant perceived', 'participant perceived anthropomorphic', 'perceived anthropomorphic chatbots', 'anthropomorphic chatbots machine', 'chatbots machine human', 'machine human existence', 'human existence verbal', 'existence verbal abuse', 'verbal abuse human', 'abuse human chatbot', 'human chatbot interaction', 'chatbot interaction expectation', 'interaction expectation chatbot', 'expectation chatbot helpfulness', 'chatbot helpfulness depending', 'helpfulness depending gender', 'depending gender dynamic', 'gender dynamic semi', 'dynamic semi structured', 'semi structured interview', 'structured interview methodology', 'interview methodology five', 'methodology five student', 'five student conducted', 'student conducted qualitative', 'conducted qualitative data', 'qualitative data collection', 'data collection collected', 'collection collected data', 'collected data manually', 'data manually analyzed', 'manually analyzed using', 'analyzed using thematic', 'using thematic analysis', 'thematic analysis result', 'analysis result study', 'result study find', 'study find empathy', 'find empathy human', 'empathy human chatbot', 'human chatbot interaction', 'chatbot interaction regardless', 'interaction regardless whether', 'regardless whether participant', 'whether participant perceive', 'participant perceive anthropomorphic', 'perceive anthropomorphic chatbots', 'anthropomorphic chatbots human', 'chatbots human machine', 'human machine however', 'machine however level', 'however level empathy', 'level empathy generally', 'empathy generally low', 'generally low participant', 'low participant become', 'participant become frustrated', 'become frustrated dissatisfied', 'frustrated dissatisfied response', 'dissatisfied response chatbots', 'response chatbots exit', 'chatbots exit chatbots', 'exit chatbots without', 'chatbots without expressing', 'without expressing frustration', 'expressing frustration usually', 'frustration usually forgot', 'usually forgot frustration', 'forgot frustration came', 'frustration came question', 'came question another', 'question another time', 'another time study', 'time study also', 'study also showed', 'also showed participant', 'showed participant may', 'participant may expect', 'may expect help', 'expect help politeness', 'help politeness chatbots', 'politeness chatbots likely', 'chatbots likely female']\n",
      "['adaptation innovation extremely', 'innovation extremely important', 'extremely important manufacturing', 'important manufacturing industry', 'manufacturing industry development', 'industry development lead', 'development lead sustainable', 'lead sustainable manufacturing', 'sustainable manufacturing using', 'manufacturing using new', 'using new technology', 'new technology promote', 'technology promote sustainability', 'promote sustainability smart', 'sustainability smart production', 'smart production requires', 'production requires global', 'requires global perspective', 'global perspective smart', 'perspective smart production', 'smart production application', 'production application technology', 'application technology regard', 'technology regard thanks', 'regard thanks intensive', 'thanks intensive research', 'intensive research effort', 'research effort field', 'effort field artificial', 'field artificial intelligence', 'artificial intelligence ai', 'intelligence ai number', 'ai number ai', 'number ai based', 'ai based technique', 'based technique machine', 'technique machine learning', 'machine learning already', 'learning already established', 'already established industry', 'established industry achieve', 'industry achieve sustainable', 'achieve sustainable manufacturing', 'sustainable manufacturing thus', 'manufacturing thus aim', 'thus aim present', 'aim present research', 'present research analyze', 'research analyze systematically', 'analyze systematically scientific', 'systematically scientific literature', 'scientific literature relating', 'literature relating application', 'relating application artificial', 'application artificial intelligence', 'artificial intelligence machine', 'intelligence machine learning', 'machine learning ml', 'learning ml industry', 'ml industry fact', 'industry fact introduction', 'fact introduction industry', 'introduction industry 4', 'industry 4 0', '4 0 artificial', '0 artificial intelligence', 'artificial intelligence machine', 'intelligence machine learning', 'machine learning considered', 'learning considered driving', 'considered driving force', 'driving force smart', 'force smart factory', 'smart factory revolution', 'factory revolution purpose', 'revolution purpose review', 'purpose review classify', 'review classify literature', 'classify literature including', 'literature including publication', 'including publication year', 'publication year author', 'year author scientific', 'author scientific sector', 'scientific sector country', 'sector country institution', 'country institution keywords', 'institution keywords analysis', 'keywords analysis done', 'analysis done using', 'done using web', 'using web science', 'web science scopus', 'science scopus database', 'scopus database furthermore', 'database furthermore ucinet', 'furthermore ucinet nvivo', 'ucinet nvivo 12', 'nvivo 12 software', '12 software used', 'software used complete', 'used complete literature', 'complete literature review', 'literature review ml', 'review ml ai', 'ml ai empirical', 'ai empirical study', 'empirical study published', 'study published last', 'published last century', 'last century carried', 'century carried highlight', 'carried highlight evolution', 'highlight evolution topic', 'evolution topic industry', 'topic industry 4', 'industry 4 0', '4 0 introduction', '0 introduction 1999', 'introduction 1999 eighty', '1999 eighty two', 'eighty two article', 'two article reviewed', 'article reviewed classified', 'reviewed classified first', 'classified first interesting', 'first interesting result', 'interesting result greater', 'result greater number', 'greater number work', 'number work published', 'work published usa', 'published usa increasing', 'usa increasing interest', 'increasing interest birth', 'interest birth industry', 'birth industry 4', 'industry 4 0']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[326], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;28mprint\u001B[39m(grams_original)\n\u001B[1;32m     11\u001B[0m         similitud \u001B[38;5;241m=\u001B[39m calculate_similarity(embeddings_plagiado, embeddings_original)\n\u001B[0;32m---> 12\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m similitud \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m:  \u001B[38;5;66;03m# Ajusta el umbral de similitud según tus necesidades\u001B[39;00m\n\u001B[1;32m     13\u001B[0m             resultados\u001B[38;5;241m.\u001B[39mappend([grams_plagiado, grams_original, similitud])\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Ordenar los resultados por similitud descendente\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "execution_count": 326
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
