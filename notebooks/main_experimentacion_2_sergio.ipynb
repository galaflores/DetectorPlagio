{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.801250Z",
     "start_time": "2024-04-21T09:43:42.782865Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import os\n",
    "\n",
    "lancStemmer = LancasterStemmer()  # stemming algorithm Lancaster\n",
    "\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer() #lemmatizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.801462Z",
     "start_time": "2024-04-21T09:43:42.787283Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palabras = [palabra.lower() for palabra in re.findall(r'\\w+', text.lower())]\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        if palabra not in stopwords:\n",
    "            text_lista.append(palabra)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.801512Z",
     "start_time": "2024-04-21T09:43:42.790012Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_stemmer(text):\n",
    "    palabras = remove_stopwords(text)\n",
    "    palabras = palabras.split()\n",
    "    text_lista = []\n",
    "    for palabra in palabras:\n",
    "        nueva = lancStemmer.stem(palabra)\n",
    "        text_lista.append(nueva)\n",
    "    nuevo_texto = ' '.join(text_lista)\n",
    "    return nuevo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.801676Z",
     "start_time": "2024-04-21T09:43:42.793460Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_grams(text, n):\n",
    "    text = get_stemmer(text)  # pre-procesa el parrafo\n",
    "    text = text.split()  # separa los caracteres pre-procesados del parrafo en listas\n",
    "    if n == 0:\n",
    "        return text\n",
    "    else:\n",
    "        grams = ngrams(text, n)  # genera los ngrams\n",
    "        result = []\n",
    "        for ng in grams:\n",
    "            result.append(' '.join(ng))  # agrega los ngrams en una lista llamada result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.818688Z",
     "start_time": "2024-04-21T09:43:42.797113Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pre_procesados (folder_path, n):\n",
    "  preprocess_texts = []\n",
    "  for fileid in os.listdir(folder_path):\n",
    "    if fileid.endswith(\".txt\"):\n",
    "      filepath = os.path.join(folder_path, fileid)\n",
    "      with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        grams = get_grams(text, n)\n",
    "        preprocess_texts.append((fileid, grams))\n",
    "  return preprocess_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:42.825513Z",
     "start_time": "2024-04-21T09:43:42.800036Z"
    }
   },
   "outputs": [],
   "source": [
    "def matriz_parrafos(grams1, grams2):\n",
    "    grams_palabras = set(grams1 + grams2)  # set de palabras de ambos ngrams\n",
    "    grams_juntos = [grams1, grams2]  # lista con ambas listas de los ngrams de cada parrafo\n",
    "    matriz = []\n",
    "    for grama in grams_juntos:\n",
    "        vector = []\n",
    "        for palabra in grams_palabras:\n",
    "            vector.append(\n",
    "                1 if palabra in grama else 0)  # compara las palabras de los grams a la palabra y agrega 1 o 0 al vector del parrafo\n",
    "        matriz.append(vector)\n",
    "    return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.395987Z",
     "start_time": "2024-04-21T09:43:42.803082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener n-gramas preprocesados\n",
    "folder_path = \"/Users/sergiogonzalez/Documents/GitHub/DetectorPlagio/textos_plagiados\" # Ruta de la carpeta con los textos plagiados\n",
    "preprocess_plagiados = pre_procesados(folder_path, 3)\n",
    "\n",
    "folder_path_og = \"/Users/sergiogonzalez/Documents/GitHub/DetectorPlagio/docs_originales\" # Ruta de la carpeta con los textos originales\n",
    "preprocess_originales = pre_procesados(folder_path_og, 3)\n",
    "\n",
    "for id_plagiado, (name_plagiado, grams_plagiado) in enumerate(preprocess_plagiados, 1):\n",
    "    print(f'\\nDocumento analizado: {name_plagiado}')\n",
    "    for id_original, (name_original, grams_original) in enumerate(preprocess_originales, 1):\n",
    "        similitud = cosine_similarity(matriz_parrafos(grams_plagiado, grams_original))\n",
    "        print(f\"Similitud de Coseno entre {name_plagiado} y {name_original}: {similitud[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB CADENAS DE OCURRENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.864513Z",
     "start_time": "2024-04-21T09:43:43.430797Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = []\n",
    "for id_plagiado, (name_plagiado, grams_plagiado) in enumerate(preprocess_plagiados, 1):\n",
    "    for id_original, (name_original, grams_original) in enumerate(preprocess_originales, 1):\n",
    "        similitud = cosine_similarity(matriz_parrafos(grams_plagiado, grams_original))\n",
    "        if similitud[0][1] != 0.0 and similitud[0][1] >= 0.2:\n",
    "            resultados.append([name_plagiado, name_original, similitud[0][1]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.868084Z",
     "start_time": "2024-04-21T09:43:43.865540Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados.sort(key=lambda x: x[2], reverse=True)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados Esperados\n",
    "1-. una lista ordenada,descendentemente por similitud, donde cada entrada incluye, para cada documento asociado.\n",
    "\n",
    "2-. puntaje de similitud.\n",
    "\n",
    "3-. cadena de coincidencia.\n",
    "\n",
    "4-. longitud y el apuntador que nos lleva a su ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.869704Z",
     "start_time": "2024-04-21T09:43:43.868153Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados.sort(key=lambda x: x[2], reverse=True)\n",
    "lista_titulos = []\n",
    "for resultado in resultados:\n",
    "    for plagiados in preprocess_plagiados:\n",
    "        if plagiados[0] == resultado[0]:\n",
    "            lista_titulos.append([plagiados[0], resultado[1]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.872446Z",
     "start_time": "2024-04-21T09:43:43.869583Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.874352Z",
     "start_time": "2024-04-21T09:43:43.873147Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def buscar_y_tokenizar(directorio, nombre_archivo):\n",
    "    for filename in os.listdir(directorio):\n",
    "        if filename == nombre_archivo:\n",
    "            filepath = os.path.join(directorio, filename)\n",
    "            with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                return sentences\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.877968Z",
     "start_time": "2024-04-21T09:43:43.876677Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def encontrar_coincidencias(sentences_originales, sentences_plagiados):\n",
    "    coincidencias = []\n",
    "\n",
    "    for sentence_orig in sentences_originales:\n",
    "        for sentence_plag in sentences_plagiados:\n",
    "            matcher = difflib.SequenceMatcher(None, sentence_orig, sentence_plag)\n",
    "            match = matcher.find_longest_match(0, len(sentence_orig), 0, len(sentence_plag))\n",
    "            if match.size > 0:\n",
    "                # Aplicar stemming y eliminar stopwords a las coincidencias antes de contar las palabras\n",
    "                cadena_orig_stemmed = get_stemmer(sentence_orig[match.a:match.a + match.size])\n",
    "                cadena_plag_stemmed = get_stemmer(sentence_plag[match.b:match.b + match.size])\n",
    "                # Contar las palabras en las coincidencias después de aplicar el stemming y eliminar las stopwords\n",
    "                palabras_orig = remove_stopwords(cadena_orig_stemmed).split()\n",
    "                palabras_plag = remove_stopwords(cadena_plag_stemmed).split()\n",
    "                if len(palabras_orig) > 1 and len(palabras_plag) > 1:  # Solo considerar coincidencias con más de una palabra\n",
    "                    coincidencias.append({\n",
    "                        \"cadena_orig\": sentence_orig[match.a:match.a + match.size],\n",
    "                        \"cadena_plag\": sentence_plag[match.b:match.b + match.size],\n",
    "                        \"longitud\": match.size\n",
    "                    })\n",
    "\n",
    "    return coincidencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:43:43.881550Z",
     "start_time": "2024-04-21T09:43:43.880358Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcular_similitud_ngramas(sentences_originales, sentences_plagiados, n):\n",
    "    grams_originales = []\n",
    "    grams_plagiados = []\n",
    "\n",
    "    for sentence in sentences_originales:\n",
    "        grams_originales.extend(get_grams(sentence, n))\n",
    "    for sentence in sentences_plagiados:\n",
    "        grams_plagiados.extend(get_grams(sentence, n))\n",
    "        \n",
    "\n",
    "    matriz = matriz_parrafos(grams_originales, grams_plagiados)\n",
    "    # Calcular la similitud coseno entre las matrices de n-gramas\n",
    "    similitud = cosine_similarity(matriz)[0][1]\n",
    "\n",
    "    return similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:04:23.392659Z",
     "start_time": "2024-04-21T10:04:23.063304Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for titulo in lista_titulos:\n",
    "    resultados = []\n",
    "    sentences_originales = buscar_y_tokenizar(folder_path_og, titulo[1])\n",
    "    sentences_plagiados = buscar_y_tokenizar(folder_path, titulo[0])\n",
    "    print(f\"Titulo: {titulo[0]}\")\n",
    "    \n",
    "    if sentences_originales and sentences_plagiados:\n",
    "        similitud = calcular_similitud_ngramas(sentences_originales, sentences_plagiados, 0) \n",
    "        print(f\"Similitud entre '{titulo[0]}' y '{titulo[1]}': {similitud * 100:.2f}%\")\n",
    "        coincidencias = encontrar_coincidencias(sentences_originales, sentences_plagiados)\n",
    "        print(f\"Coincidencias para '{titulo[0]}' y '{titulo[1]}':\")\n",
    "        \n",
    "        for coincidencia in coincidencias:\n",
    "            print(f\"Cadena original: {coincidencia['cadena_orig']} (Longitud: {coincidencia['longitud']})\")\n",
    "            print(f\"Cadena plagiada: {coincidencia['cadena_plag']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"No se encontraron oraciones en '{titulo[0]}' o '{titulo[1]}'\")\n",
    "    print(\"----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:10:29.459811Z",
     "start_time": "2024-04-21T10:10:23.163145Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:16:50.589439Z",
     "start_time": "2024-04-21T10:16:50.584912Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crear_documento_pdf(titulo, similitud, coincidencias):\n",
    "    # Crear un nuevo objeto PDF\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    # Agregar una página\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Establecer la fuente y el tamaño del texto\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Título del documento\n",
    "    pdf.cell(200, 10, txt=f\"Resultados de prueba de plagio: {titulo[1]}\", ln=True, align=\"C\")\n",
    "\n",
    "    # Plagio detectado\n",
    "    pdf.cell(200, 10, txt=f\"Plagio detectado: {similitud * 100:.2f}%\", ln=True, align=\"C\")\n",
    "    pdf.ln(5)\n",
    "\n",
    "    # Texto con resaltado de las coincidencias\n",
    "    for coincidencia in coincidencias:\n",
    "        cadena_orig = coincidencia['cadena_orig']\n",
    "        cadena_plag = coincidencia['cadena_plag']\n",
    "        # Verificar si existe la clave 'referencia'\n",
    "        if 'referencia' in coincidencia:\n",
    "            referencia = coincidencia['referencia']\n",
    "            texto = f\"Texto original: {cadena_orig}\\nTexto plagiado: {cadena_plag}\\nReferencia: {referencia}\\n\\n\"\n",
    "        else:\n",
    "            texto = f\"Texto original: {cadena_orig}\\nTexto plagiado: {cadena_plag}\\n\\n\"\n",
    "        pdf.set_font(\"Arial\", style=\"B\")\n",
    "        pdf.multi_cell(0, 10, txt=texto, border=1, align=\"L\")\n",
    "\n",
    "    # Guardar el documento PDF en la carpeta de resultados\n",
    "    nombre_archivo = f\"Resultado_similitud_{titulo[0]}_y_{titulo[1]}.pdf\"\n",
    "    ruta_archivo = os.path.join(\"/Users/sergiogonzalez/Documents/GitHub/DetectorPlagio/app/Resultados\", nombre_archivo)\n",
    "    pdf.output(ruta_archivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:16:52.685435Z",
     "start_time": "2024-04-21T10:16:52.178591Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for titulo in lista_titulos:\n",
    "    resultados = []\n",
    "    sentences_originales = buscar_y_tokenizar(folder_path_og, titulo[1])\n",
    "    sentences_plagiados = buscar_y_tokenizar(folder_path, titulo[0])\n",
    "\n",
    "    if sentences_originales and sentences_plagiados:\n",
    "        similitud = calcular_similitud_ngramas(sentences_originales, sentences_plagiados, 0) \n",
    "        coincidencias = encontrar_coincidencias(sentences_originales, sentences_plagiados)\n",
    "            \n",
    "        # Llamar a la función para crear el documento PDF\n",
    "        crear_documento_pdf(titulo, similitud, coincidencias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
